%!TEX root = pfc-memoria.tex
%!TEX encoding = UTF-8 Unicode

\chapter{Aprendizaje automático}

El \textbf{aprendizaje automático}\index{aprendizaje automático} (en inglés, \emph{machine learning}\index{machine learning@\emph{machine learning}}) es la rama de la Inteligencia Artificial cuyo objetivo es desarrollar técnicas que permitan a las computadoras \textbf{aprender}. De forma más concreta, se trata de crear programas capaces de generalizar comportamientos a partir de una información suministrada en forma de ejemplos de la que debe extraer el conocimiento necesario para que el programa pueda realizar automáticamente la tarea que se le pretende \citep[Aprendizaje automático]{wikipedia-es}.

Estas posibles tareas pueden ser: \citep{MartinMateos2013} 
\begin{itemize}
\item Toma de decisiones
\item Clasificación de documentos
\item Búsqueda de documentos relevantes
\item Comprensión del lenguaje
\item Recuperación de la información
\item Extracción de la información
\item Generación de discurso
\item Traducción automática
\item \ldots
\end{itemize}

\section{Concepto}

Vamos a explicar el concepto de método de aprendizaje automático para tareas de clasificación de documentos.
Formalizando el \nombrebf{problema de clasificación}\index{clasificación!problema de}, tenemos la descripción $d\in \mathbb{X}$ de un documento, dónde $\mathbb{X}$ es el espacio de documentos; y un conjunto finito de clases $\mathbb{C} = \{c_1, c_2, \ldots, c_J\}$. Estas clases también se llaman categorías o etiquetas \citep{Manning2008}.

$\mathbb{X}$ suele ser un espacio vectorial con un gran número de dimensiones, mientras que las clases son las necesarias para la aplicación. En el caso de detección de la polaridad del sentimiento, podría ser $\mathbb{C}=\{-2,-1,0,-1,-2\}$, representando a opiniones muy negativas, negativas, neutras, positivas y muy positivas.

En lugar de tener que programar manualmente cómo relacionar documentos y clases para construir el clasificador, tomaremos un conjunto de entrenamiento $\mathbb{D}$ de documentos $d$ manualmente etiquetados con su clase $\langle d, c \rangle \in \mathbb{X} \times \mathbb{C}$.

Usando un método de \nombrebf{aprendizaje automático}\index{aprendizaje automático} $\Gamma$ aplicado al conjunto de entrenamiento $\Gamma(\mathbb{D})$ obtenemos como resultado el clasificador $\gamma : \mathbb{X} \mapsto \mathbb{C}$ \citep{Manning2008} que predice la clase a la que pertenecen documentos no vistos en la fase de entrenamiento.

\section{Métodos de aprendizaje automático}

Existen básicamente dos grupos de métodos de aprendizaje automático:
\nopagebreak
\begin{description}
\item[Aprendizaje supervisado]\index{aprendizaje!supervisado} Estos algoritmos producen una función entrenada con un conjunto de entrenamiento que proporciona la entrada y también la salida (la \emph{clase}, en el caso del problema de clasificación) que se desea para el sistema.
\item[Aprendizaje no supervisado]\index{aprendizaje!no supervisado} Éstos, por el contrario, se entrenan simplemente con las entradas (en el caso de clasificación: sin necesidad de estar la muestra etiquetada con su clase).
\item[Aprendizaje semi-supervisado]\index{aprendizaje!semi-supervisado} Combina ambos tipos de aprendizaje.
\end{description}

A modo de muestra, enumeramos algunos de los algoritmos para cada tipo:
\nopagebreak
\begin{itemize}
\item Supervisados:
\begin{itemize}
\item Árboles de decisión
\item Vecinos más cercanos
\item Aprendizaje por refuerzo con retroalimentación (ensayo-error)
\item Máquinas de Vectores de Soporte (\emph{Support Vector Machines}, SVM\index{SVM})
\item Aprendizaje estadístico Naïve-Bayes
\item Redes Neuronales Artificiales
\end{itemize}
\item No supervisados:
\begin{itemize}
\item Agrupamiento plano: \emph{flat partitional clustering}
\item Agrupamiento jerárquico: \emph{hierarchical clustering}
\end{itemize}
\end{itemize}

En las figuras \ref{fig:hier-clustering} y \ref{fig:k-medias} se puede ver las diferencias en las etapas de clustering para el método de particionado plano y el jerárquico.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.6\textwidth]{hierarchical-clustering}
\caption[Etapas del agrupamiento jerárquico]{Etapas del agrupamiento jerárquico \citep{Rai2011}}
\label{fig:hier-clustering}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.6\textwidth]{k-medias}
\caption[Etapas del agrupamiento plano]{Etapas del agrupamiento plano \citep{RuizReina2013}}
\label{fig:k-medias}
\end{figure}

Además, uno de los retos de estos sistemas es cómo representar el conocimiento extraído en el entrenamiento para almacenarlo y usarlo posteriormente.

\section{Extracción de características}

En los métodos de aprendizaje que vemos en esta sección, por regla general hay que establecer un vector que represente a cada documento (frase). Todos los vectores deberán tener la misma dimensión, y cada dimensión del espacio vectorial de documentos recibe el nombre de \nombrebf{característica}\index{característica} \emph{(feature)}\index{feature@\emph{feature}}. De esta manera, un documento se equipara a un vector de valores numéricos (números en coma flotante, bien en cualquier rango o bien normalizado y acotado en el intervalo $[0,1]$); lo que se puede representar mediante un \emph{punto} dentro del espacio vectorial (\autoref{fig:feat-space}).

\begin{figure}[htbp]
\centering
\todo{pendiente dibujar mejor}
\includegraphics[width=0.6\textwidth]{feat-space}
\caption{Espacio vectorial de las características de los documentos \emph{(feature document space)}}
\label{fig:feat-space}
\end{figure}

Una vez transformado el corpus de texto de entrenamiento en un conjunto de puntos dentro del espacio vectorial, se pasa como entrada al método de aprendizaje automático. Si estos puntos llevan asociada la clase, será un aprendizaje supervisado. En caso contrario, será un aprendizaje no supervisado.

Antes de proceder al extracción de las características del texto del documento, es útil aplicar algún procesamiento como los explicados en la \autoref{sec:preproc-NLP}, ``\nameref{sec:preproc-NLP}''.

\subsection{Saco de palabras \emph{(bag-of-words)}}

\begin{definition}[saco de palabras]
Se denomina \nombrebf{saco de palabras}\index{saco de palabras} (BOW, por sus siglas en inglés de \emph{bag-of-words})\index{bag-of-words@\emph{bag-of-words}} al conjunto de palabras existente en un documento.
\end{definition}

A este saco, se le pueden eliminar las palabras vacías (stopwords) ---si es que no se le ha aplicado antes---, y además no tener en cuenta las palabras (términos) con poca frecuencia de distribución dentro del conjunto de corpus si éste es muy grande, como manera de reducir el vocabulario y mejorar el rendimiento o reducir el \nombrebf{sobreajuste}\index{sobreajuste} \emph{(overfitting)}\index{overfitting@\emph{overfitting}} del modelo.

\begin{definition}[vocabulario]
El \nombrebf{vocabulario}\index{vocabulario} es el conjunto formado por los términos que se toman en consideración para una aplicación de NLP. Se puede contemplar el uso del vocabulario común del idioma, también llamado \nombrebf{lexicón}\index{lexicón}, diccionario o tesauro\index{tesauro}.
\end{definition}

Una vez calculado el vocabulario, se usa cada término como característica, y o bien se indica con un valor 0 ó 1 si el término está presente en el documento o no, o bien el conteo del número de las apariciones del término en el mismo.

\begin{example}[Vocabulario y representación usando BOW]\label{exa:ml-bow}
Dado el siguiente conjunto de documentos:
\begin{enumerate}
\item ``A series of escapades demonstrating the adage that what is good for the goose''
\item ``demonstrating the adage that what is good for the goose''
\item ``amuses but none of which amounts to much of a story''
\end{enumerate}
Obtener el vocabulario de características y vector de representación BOW para cada documento.
\end{example}

La solución al ejercicio la calculamos con en el \autoref{lst:vocab-feat-BOW}.

\begin{listing}[htbp]
\begin{minted}{python}
>>> doc1 = "A series of escapades demonstrating the adage that what is good for the goose".split(' ')
>>> doc2 = "demonstrating the adage that what is good for the goose".split(' ')
>>> doc3 = "amuses but none of which amounts to much of a story".split(' ')
>>> features = list(set(doc1 + doc2 + doc3))
>>> features.sort()
>>> vec1 = [doc1.count(feat) for feat in features]
>>> vec2 = [doc2.count(feat) for feat in features]
>>> vec3 = [doc3.count(feat) for feat in features]
>>> print(features) # vocabulario de características
['A', 'a', 'adage', 'amounts', 'amuses', 'but', 'demonstrating', 'escapades', 'for', 'good', 'goose', 'is', 'much', 'none', 'of', 'series', 'story', 'that', 'the', 'to', 'what', 'which']
>>> print(vec1) # vector de representación del documento 1
[1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 2, 0, 1, 0]
>>> print(vec2) # vector de representación del documento 2
[0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0]
>>> print(vec3) # vector de representación del documento 3
[0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 2, 0, 1, 0, 0, 1, 0, 1]
>>> 
\end{minted}
\caption{Vocabulario de características y representación BOW}
\label{lst:vocab-feat-BOW}
\end{listing}

\subsection{Saco de bigramas \emph{(2-BOW)}}

Análogo al anterior, pero considerando como términos:
\begin{itemize}
\item Cada palabra.
\item Cada par de palabras consecutivas (bigrama).
\end{itemize}

\begin{example}[Vocabulario y representación usando 2-BOW]\label{exa:ml-2bow}
Dado el siguiente conjunto de documentos:
\begin{enumerate}
\item ``A series of escapades demonstrating the adage that what is good for the goose''
\item ``demonstrating the adage that what is good for the goose''
\item ``amuses but none of which amounts to much of a story''
\end{enumerate}
Obtener el vocabulario de características y vector de representación 2-BOW para cada documento.
\end{example}

La solución al ejercicio la calculamos en el \autoref{lst:vocab-feat-2BOW}.

\begin{listing}[htbp]
\begin{minted}{python}
>>> from nltk.util import bigrams
>>> doc1 = "A series of escapades demonstrating the adage that what is good for the goose".split(' ')
>>> doc2 = "demonstrating the adage that what is good for the goose".split(' ')
>>> doc3 = "amuses but none of which amounts to much of a story".split(' ')
>>> bg1 = list(bigrams(doc1)) + [(w,) for w in doc1]
>>> bg2 = list(bigrams(doc2)) + [(w,) for w in doc2]
>>> bg3 = list(bigrams(doc3)) + [(w,) for w in doc3]
>>> features = list(set(bg1 + bg2 + bg3))
>>> features.sort()
>>> vec1 = [bg1.count(feat) for feat in features]
>>> vec2 = [bg2.count(feat) for feat in features]
>>> vec3 = [bg3.count(feat) for feat in features]
>>> print(features) # vocabulario de características
[('A',), ('A', 'series'), ('a',), ('a', 'story'), ('adage',), ('adage', 'that'), ('amounts',), ('amounts', 'to'), ('amuses',), ('amuses', 'but'), ('but',), ('but', 'none'), ('demonstrating',), ('demonstrating', 'the'), ('escapades',), ('escapades', 'demonstrating'), ('for',), ('for', 'the'), ('good',), ('good', 'for'), ('goose',), ('is',), ('is', 'good'), ('much',), ('much', 'of'), ('none',), ('none', 'of'), ('of',), ('of', 'a'), ('of', 'escapades'), ('of', 'which'), ('series',), ('series', 'of'), ('story',), ('that',), ('that', 'what'), ('the',), ('the', 'adage'), ('the', 'goose'), ('to',), ('to', 'much'), ('what',), ('what', 'is'), ('which',), ('which', 'amounts')]
>>> print(vec1) # vector de representación del documento 1
[1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 2, 1, 1, 0, 0, 1, 1, 0, 0]
>>> print(vec2) # vector de representación del documento 2
[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 1, 1, 0, 0, 1, 1, 0, 0]
>>> print(vec3) # vector de representación del documento 3
[0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1]
>>> 
\end{minted}
\caption{Vocabulario de características y representación 2-BOW}
\label{lst:vocab-feat-2BOW}
\end{listing}


 


\section{Clasificadores}

Los clasificadores son programas que dado un documento de entrada, le asocia una categoría o etiqueta. Estos clasificadores han debido ser previamente entrenados mediante una técnica de aprendizaje automático, o bien programados con las reglas apropiadas.

En función del resultado de la clasificación, podemos distinguir dos tipos de clasificadores:
\nopagebreak
\begin{description}
\item[Clasificador binario] En este caso, existen dos posibles categorías a asocias a cada documento. Por ejemplo, un detector de \emph{spam} puede determinar que un mensaje de correo electrónico es spam o no lo es (es \emph{spam} o es \emph{ham}). El resultado puede venir ---y es recomendable---, acompañado de un valor de confidencia (probabilidad de haber acertado la predicción).
\item[Clasificador múltiple] Aquí el documento puede asignarse a varias categorías, con un valor de confidencia diferente para cada una. Se podría posteriormente tener en cuenta solamente la categoría asignada con mayor probabilidad, o bien se infiere que pertenece simultáneamente a las $n$ categorías con mayor probabilidad (por ejemplo, en un clasificador de temática, cuando nos encontramos con un documento que versa sobre dos temáticas diferentes).
\end{description}


\subsection{Bondad del clasificador (valor-F)}

Para la comparación de sistemas de búsqueda y recuperación de información disponemos de una métrica llamada valor-F\index{valor-F} (\emph{F-score}\index{F-score@\emph{F-score}}, \emph{F-measure}\index{F-measure@\emph{F-measure}} ó \emph{$F_1$ score}\index{F\_1 score@$F_1$ score}) siendo ésta la media armónica de otros dos indicadores \citep[Precisión y exhaustividad]{wikipedia-es}:
\begin{description}
\item[precisión \emph{(precision)}] \index{precisión}\index{precision@\emph{precision}}
Es la fracción de instancias recuperadas que son relevantes.
\begin{eqnarray}
\text{precisión} &=& \frac{|\{\text{relevantes}\}\cap\{\text{recuperados}\}|}{|\{\text{recuperados}\}|}
\end{eqnarray}
\item[exhaustividad \emph{(recall)}] \index{exhaustividad}\index{recall@\emph{recall}}
Es la fracción de instancias relevantes que han sido recuperadas.
\begin{eqnarray}
\text{exhaustividad} &=& \frac{|\{\text{relevantes}\}\cap\{\text{recuperados}\}|}{|\{\text{relevantes}\}|}
\end{eqnarray}
\item[valor-F \emph{(F-score)}] Media armónica de precisión y exhaustividad.
\begin{eqnarray}
F_1 &=& 2\times\frac{\text{precisión}\times\text{exhaustividad}}{\text{precisión}+\text{exhaustividad}}
\end{eqnarray}
\end{description}

Supongamos un clasificador binario:
\begin{itemize}
\item La medida de precisión será el porcentaje de documentos correctamente clasificados dentro del conjunto de prueba.
\item La medida de exhaustividad será el porcentaje de documentos del conjunto de referencia que han sido correctamente clasificados \citep{Perkins2010}.
\end{itemize}

\subsection{Naïve Bayes multinomial}

Éste es un clasificador de los llamados estadísticos o probabilísticos. En la fase de aprendizaje supervisado de este clasificador \citep{Jurafsky2015}:
\begin{itemize}
\item Se construye un vocabulario $V$ de los términos vistos en todos los ejemplos.
\item Se aprenden las probabilidades a priori $\mathcal{P}(c)$ de que un documento cualquiera pertenezca a una clase $c$. Esto se hace simplemente contando el número de ejemplos que pertenecen a cada clase, referido del total de ejemplos suministrados.
\begin{equation}
\mathcal{P}(c) = \frac{\text{\#(ejemplos con clase $c$)}}{\text{\#(ejemplos)}}
\end{equation}
\item La verosimilitud (\emph{likelihood}) $\mathcal{P}(w|c)$ del término $w$ para cada clase $c$. Para ello se cuenta el número de apariciones del término $w$ en todos los ejemplos categorizados con clase $c$, referido del número de términos en todos los ejemplos categorizados con clase $c$.\\
Para luego no tener problemas con el estimador, le añadimos un \nombrebf{suavizado de Laplace}\index{Laplace!suavizado de} (\emph{Laplace} o \emph{add-one smoothing})\index{Laplace!smoothing@\emph{smoothing}}\index{add-one@\emph{add-one}} para que los términos no vistos durante el entrenamiento (y por lo tanto, no existentes en el vocabulario) no perjudiquen al cómputo general de la estimación.
\begin{equation}
\mathcal{P}(w|c) = \frac{\text{\#(apariciones de $w$ en ejemplos con clase $c$)} + 1}%
{\text{\#(términos en ejemplos con clase $c$)} \cdot |V|}
\end{equation}
\end{itemize}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.5\textwidth]{multinomialNB-clsfy}
\caption[Clasificador Naïve Bayes]{Clasificador Naïve Bayes \citep{Jurafsky2015}}
\label{fig:multinomialNB-clsfy}
\end{figure}

Durante la fase de clasificación, se calculan las probabilidades de que el documento pertenezca a cada clase, tomando como cierta la clase con mayor probabilidad.
\begin{equation}
c_\text{MAP} = \argmax_{c\in C} \mathcal{P}(c|d) 
= \argmax_{c\in C} \frac{\mathcal{P}(d|c)\mathcal{P}(c)}{\mathcal{P}(d)}
= \argmax_{c\in C} \mathcal{P}(d|c)\mathcal{P}(c)
\end{equation}

El documento $d$, lo representamos por su vector de características, así
\begin{equation}
c_\text{MAP} = \argmax_{c\in C} \mathcal{P}(x_1,x_2,\ldots,x_n|c)\mathcal{P}(c)
\end{equation}

Esta transformación implica que
\begin{enumerate}
\item La posición de los términos no tiene relevancia. Por tanto, sería indistinguible de cualquier otra permutación del documento o del vector de características. 
\item Las probabilidades de cada característica $x_i$ son mutuamente independientes dada una clase $c$.
\end{enumerate}

De esta manera, podemos calcular la probabilidad conjunta $\mathcal{P}(x_1,x_2,\ldots,x_n|c)$ como
\begin{equation}
\mathcal{P}(x_1,x_2,\ldots,x_n|c) = \mathcal{P}(x_1|c) \cdot \mathcal{P}(x_2|c) \cdots \mathcal{P}(x_n|c)
\end{equation}

Y finalmente
\begin{equation}
c_\text{NB} = \argmax_{c\in C} \mathcal{P}(c) \prod_{x\in X} \mathcal{P}(x|c)
\end{equation}

Lo ponemos en práctica en el siguiente ejemplo.

\begin{example}[Clasificación NB usando BOW y 2-BOW]
Dados los documentos y los vectores de características del Ejemplo~\ref{exa:ml-bow} y Ejemplo~\ref{exa:ml-2bow}, realizar ambos entrenamientos y obtener la estimación con sus probabilidades usando el clasificador NB multinomial para los siguientes documentos de test:
\begin{enumerate}
\item ``demonstrating the adage''
\item ``demonstrating the adage amuses but none of which amounts to much of a story''
\end{enumerate}
\end{example}

La solución al ejercicio la calculamos en el \autoref{lst:multinb-bow} y \autoref{lst:multinb-2bow}. El resumen de los resultados de las predicciones puede verse en la \autoref{tbl:multinb}

%guille
\begin{table}[htbp]
\centering
\begin{tabular}{|r|S[table-format=1.6]|S[table-format=1.6]|S[table-format=1.6]|S[table-format=1.6]|}
\hline
\multirow{2}{*}{\nombrebf{Doc.}} & \multicolumn{2}{|c|}{\nombrebf{BOW}} & \multicolumn{2}{|c|}{\nombrebf{2-BOW}} \\ \hhline{~----}
  & \multicolumn{1}{|c|}{$c_1$} & \multicolumn{1}{|c|}{$c_2$} & \multicolumn{1}{|c|}{$c_1$} & \multicolumn{1}{|c|}{$c_2$} \\ \hhline{=====}
1 & 0.029215 & 0.970784 & 0.006114 & 0.993885 \\ \hline
2 & 0.999253 & 0.000746 & 0.999999 & 0.000000 \\ \hline
\end{tabular}
\caption{Predicción de clasificación en Naïve Bayes}
\label{tbl:multinb}
\end{table}

\begin{listing}[htbp]
\begin{minted}{python}
>>> from sklearn.naive_bayes import MultinomialNB 
>>> doctest1 = "demonstrating the adage".split(' ')
>>> doctest2 = "demonstrating the adage amuses but none of which amounts to much of a story".split(' ')
>>> vectest1 = [doctest1.count(feat) for feat in features]
>>> vectest2 = [doctest2.count(feat) for feat in features]
>>> print(vectest1) # vector de características del documento de prueba 1 en BOW
[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
>>> print(vectest2) # vector de características del documento de prueba 2 en BOW
[0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 2, 0, 1, 0, 1, 1, 0, 1]
>>> clsfy = MultinomialNB()
>>> clsfy.fit([vec1, vec2, vec3], [2, 2, 1])
MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)
>>> print(clsfy.classes_)
[1 2]
>>> print(clsfy.predict_proba(vectest1)) # predicción de vectest1
[[ 0.02921541  0.97078459]]
>>> print(clsfy.predict_proba(vectest2)) # predicción de vectest2
[[  9.99253463e-01   7.46536777e-04]]
>>> 
\end{minted}
\caption{Predicción NB en BOW}
\label{lst:multinb-bow}
\end{listing}

\begin{listing}[htbp]
\begin{minted}{python}
>>> from nltk.util import bigrams
>>> from sklearn.naive_bayes import MultinomialNB 
>>> doctest1 = "demonstrating the adage".split(' ')
>>> doctest2 = "demonstrating the adage amuses but none of which amounts to much of a story".split(' ')
>>> bg1 = list(bigrams(doctest1)) + [(w,) for w in doctest1]
>>> bg2 = list(bigrams(doctest2)) + [(w,) for w in doctest2]
>>> vectest1 = [bg1.count(feat) for feat in features]
>>> vectest2 = [bg2.count(feat) for feat in features]
>>> print(vectest1) # vector de características del documento de prueba 1 en 2-BOW
[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0]
>>> print(vectest2) # vector de características del documento de prueba 2 en 2-BOW
[0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1]
>>> clsfy = MultinomialNB()
>>> clsfy.fit([vec1, vec2, vec3], [2, 2, 1])
MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)
>>> print(clsfy.classes_)
[1 2]
>>> print(clsfy.predict_proba(vectest1)) # predicción de vectest1
[[ 0.00611419  0.99388581]]
>>> print(clsfy.predict_proba(vectest2)) # predicción de vectest2
[[  9.99999838e-01   1.62105831e-07]]
\end{minted}
\caption{Predicción NB en 2-BOW}
\label{lst:multinb-2bow}
\end{listing}


\subsection{Máquinas de vectores de soporte}
