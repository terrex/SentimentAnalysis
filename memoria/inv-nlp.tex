%!TEX root = pfc-memoria.tex
%!TEX encoding = UTF-8 Unicode

\chapter{Procesamiento del lenguaje natural}

\epigraph{``Pero si el pensamiento corrompe el lenguaje, el lenguaje también puede corromper el pensamiento.''}{\textsc{George Orwell}}

\section{Lenguaje natural vs lenguajes formales}

En general, un \nombrebf{lenguaje}\index{lenguaje} es el conjunto, potencialmente infinito, de oraciones y secuencias de palabras construidas mediante las reglas gramaticales (en sus distintos niveles), fonéticas y de significación que rigen el propio lenguaje.

Hay diferentes tipos de lenguajes, y se pueden encontrar diferentes clasificaciones según diferentes criterios. Atendiendo su propósito y forma en que se originó:
\nopagebreak
\begin{itemize}
\item Lenguaje natural
\item Lenguaje formal
\item Lenguaje artificial o construido
\end{itemize}

\FloatBarrier
\subsection{Lenguaje natural}

La \nombrebf{lengua natural}\index{lenguaje!natural} es la lengua o idioma que nace espontáneamente de un grupo de hablantes por la mera necesidad de establecer comunicación verbal. Está ligado a la cultura de cada civilización y evoluciona según su uso por parte de su comunidad de hablantes, que debe ponerse de acuerdo en la manera de usar el lenguaje para que cualquier hablante del mismo pueda interpretar y producir mensajes con el mismo sentido en que se originó. Son los idiomas de uso común para expresar ideas y sentimientos: inglés, francés, ruso, latín, griego,~\ldots

Es muy importante enmarcar el ámbito de empleo de la lengua natural en la comunicación humana. Es evidente que en los avances técnicos en la tarea de «humanizar» las interfaces de comunicación hombre-máquina (HMI\index{HMI} por sus siglas en inglés, \emph{Human-Machine Interface}, o también HCI:\index{HCI} \emph{Human-Computer Interaction}) se intente emular la comprensión y producción verbal en el mismo lenguaje natural que los humanos; de manera que surge la necesidad de modelar el proceso de comunicación.

Los primeros estudios y experimentos se enfocaron en proporcionar el reconocimiento de órdenes escritas en lenguaje natural del inglés para comunicarse con los programas. Así tenemos por ejemplo el programa \nombrebf{STUDENT}\index{STUDENT@\emph{STUDENT}} \citep{Bobrow1964}.

Este programa realizado en \nombrebf{LISP}, posee una reglas de interpretación de ciertas construcciones en inglés, además de unas reglas básicas de conocimiento (la fórmula del cálculo de la velocidad a partir de la distancia y el tiempo, conversiones entre unidades de medida, etc.). Por ejemplo, una posible entrada sería:
\foreignblockquote{english}{Tom is twice Mary's age, and Jane's age is half the difference between \\
Mary and Tom.\\
If Mary is 18 years old, how old is Jane?}

El programa traduce la entrada en ecuaciones, las añade a sus ecuaciones integradas en su base de conocimiento inicial, y resuelve el sistema usando álgebra convencional (cuando sea posible hacerlo). Posteriormente muestra el resultado con un listado de las variables incógnitas y sus valores calculados. En el ejemplo del \autoref{lst:STUDENT}, el sistema ya conoce el significado de duplicar una cantidad (\codet{twice}), de elevar al cuadrado (\codet{square}) y cómo tratar los porcentajes (\verb=%=).

\begin{listing}[htbp]
\begin{minted}{text}
> (student '(If the number of customers Tom gets is twice the square of
             20% of the number of advertisements he runs |,|
             and the number of advertisements is 45 |,|
             then what is the number of customers Tom gets ?))

The equations to be solved are:
  CUSTOMERS = (2 * (((20 / 100) * ADVERTISEMENTS) *
                    ((20 / 100) * ADVERTISEMENTS)))
  ADVERTISEMENTS = 45
  WHAT = CUSTOMERS

The solution is:
  WHAT = 162
  CUSTOMERS = 162
  ADVERTISEMENTS = 45
NIL
\end{minted}
\caption[Ejemplo de problema descrito en lenguaje natural y su resolución en una sesión en el programa \nombrebf{STUDENT}]{Ejemplo de problema descrito en lenguaje natural y su resolución en una sesión en el programa \nombrebf{STUDENT} \citep{Norvig2014}}
\label{lst:STUDENT}
\end{listing}

\citet{Russell2003} resumen el proceso de comunicación en 7 etapas desde que el emisor toma la iniciativa de transmitir una idea al receptor, hasta que éste recibe la realización del mensaje en una idea percibida (que puede coincidir en mayor o menor medida con la idea original) para posteriormente integrarla en su conocimiento si lo estima oportuno (\autoref{fig:steps-comm-process}).

\begin{figure}[htbp]
\centering
\begin{framed}
\begin{minipage}{\textwidth}
\begin{center}
{\large \textsf{Etapas del Proceso de Comunicación}}
\end{center}
\begin{description}
\item[Escenario:] $A$lice $\longrightarrow$ $B$ob; proposición $P$; frase $W$
\item[Proceso del emisor $A$]:
\begin{enumerate}
\item \nombrebf{Intención:} $A$ quiere transmitir la proposición $P$ a $B$
\item \nombrebf{Generación:} $A$ transforma $P$ en la frase $W$
\item \nombrebf{Síntesis:} $A$ envía $W'$ a $B$
\suspend{enumerate}
\item[Proceso del receptor $B$]:
\resume{enumerate}
\item \nombrebf{Percepción:} $B$ percibe $W''$ y decodifica $W_2$
\item \nombrebf{Análisis:} $B$ infiere que $W_2$ puede tener $P_1, P_2, \ldots, P_n$ significados
\item \nombrebf{Desambiguación:} $B$ decide que la proposición $P_i$ es el significado más probable
\item \nombrebf{Incorporación:} $B$ decide si cree o no en $P_i$ y la integra
\end{enumerate}
\end{description}
\end{minipage}
\end{framed}
\caption[Etapas del Proceso de Comunicación]{Etapas del Proceso de Comunicación \citep[pp. 792]{Russell2003}}
\label{fig:steps-comm-process}
\end{figure}

Aquí $W'$ y $W''$ representan el mensaje oral o escrito físicamente de la oración, en la que es necesario la codificación $W'$ en fonemas articulados mediante las cuerdas vocales, o trazos de símbolos en un papel; y posterior decodificación $W''$ mediante el oído o la vista. La dificultad general del análisis estriba en que el lenguaje natural es propenso a ser ambiguo, y debe reducirse esa ambigüedad para procesar la idea de manera precisa y fidedigna. En la comunicación personal puede intervernir incluso el lenguaje gestual para resolver la ambigüedad; o llegar a ser en sí mismo la manera de comunicación, como en la lengua de signos que emplean las personas sordas.

Las causas de la ambigüedad pueden ser:
\nopagebreak
\begin{description}
\item[Imprecisiones semánticas.] En los lenguajes naturales no se da una correspondencia biunívoca entre signos y objetos representados: hay palabras que son demasiado vagas, que están mal definidas. Otras tienen más de un significado (son polisémicas), y se pueden usar ambiguamente.
\item[Impresiones sintácticas.] Las reglas gramaticales permiten formar enunciados que no tienen ningún significado, aunque sean sintácticamente correctas y se pueda desarrollar un árbol de derivación.
\end{description}

Este lenguaje que nos resulta tan útil para la vida cotidiana sin embargo, no es adecuado para la ciencia, que necesita rigor y precisión. \citep{DiazSantos2015}

\FloatBarrier
\subsection{Lenguaje formal}

Los \nombrebf{lenguajes formales}\index{lenguaje!formal}, al contrario que los naturales, difieren en que no han surgido espontáneamente, sino que han sido diseñados para un ámbito de aplicación concreto (normalmente en las ciencias) y se definen de manera que sean precisos y libres de cualquier ambigüedad.

Ejemplos de lenguas formales son:
\nopagebreak
\begin{itemize}
\item Lenguaje matemático y lógico
\item Lenguajes de programación (C, Java, Fortran,~\ldots)
\item Lenguaje musical
\end{itemize}

Formalmente, un lenguaje es el conjunto (potencialmente infinito) de oraciones (sentencias o frases) que se pueden construir con las reglas de la gramática asociada al lenguaje.

\begin{definition}[Gramática formal]\index{gramática!formal}
\citep{MorenoVelo2010}
Se denomina \nombrebf{gramática formal} $G$ la gramática definida por la cuádrupla
\[
G = \left< \Sigma_T, \Sigma_N, S, P \right>
\]
\begin{itemize}
\item $\Sigma_T$: alfabeto de símbolos terminales
\item $\Sigma_N$: alfabeto de símbolos no terminales
\item $S \in \Sigma_N$: símbolo inicial de la gramática
\item $P$: conjunto finito de reglas de producción $u \rightarrow v$, con $u \in \Sigma^+$ y $v \in \Sigma^*$
\end{itemize}
con el alfabeto $\Sigma = \Sigma_T \cup \Sigma_N$; siendo éstos disjuntos: $\Sigma_T \cap \Sigma_N = \varnothing$
\end{definition}

Dependiendo de las restricciones en la forma de las reglas de producción, \citet{Chomsky1965} enumera 4 tipos de gramáticas generativas, que generan 4 tipos de lenguajes, cada uno con mayor nivel de expresividad (y por tanto siendo necesario una máquina con mayor potencia de cálculo para ser procesado) conocido como la \nombrebf{Jerarquía de Chomsky}\index{Chomsky!jerarquía de}\index{jerarquía!de Chomsky} (\autoref{tbl:jerarquia-chomsky}).

\begin{table}[htbp]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
\nombrebf{tipo} & \nombrebf{lenguaje} & \nombrebf{autómata} & \nombrebf{reglas} \\ \hhline{====}
L0 & recursivamente enumerable (LRE)\index{LRE} & máquina de Turing (MT) & Sin restricciones \\ \hline
L1 & sensible al contexto (LSC)\index{LSC} & autómata linealmente acotado &
$\alpha A \beta \rightarrow \alpha \gamma \beta$ \\ \hline
L2 & libre de contexto (LLC)\index{LLC} & autómata con pila & $A \rightarrow \gamma$ \\ \hline
L3 & regular (LR)\index{LR} & autómata finito & $A \rightarrow aB$; $A \rightarrow a$ \\ \hline
\end{tabular}
\caption[Jerarquía de Chomsky]{Jerarquía de Chomsky \citep{Chomsky1965}}
\label{tbl:jerarquia-chomsky}
\end{table}

Los lenguajes formales de nivel inferior se encuentran estrictamente contenidos en los lenguajes de nivel superior (\autoref{eqn:conjuntos-chomsky}).
\begin{eqnarray}
\text{L3} \subset \text{L2} \subset \text{L1} \subset \text{L0}
\label{eqn:conjuntos-chomsky}
\end{eqnarray}

La complejidad en el procesamiento aumenta conforme aumenta la potencia de expresividad:
\nopagebreak
\begin{itemize}
\item L3: lineal
\item L2: polinómica
\item L1: exponencial
\item L0: indecible
\end{itemize}

\FloatBarrier
\subsection{Lenguaje artificial o construido}

Las \nombrebf{lenguas artificiales}\index{lenguaje!artificial} son lenguas \emph{a priori}, diseñadas antes de ser usadas por sus parlantes. Se distingue de las lenguas formales en que no se exige una precisión libre de ambigüedad, ya que su ámbito de aplicación es el mismo que el lenguaje natural, siendo aceptable la polisemia, homofonía, etc.

Dentro de las lenguas artificiales, las podemos clasificar en:
\nopagebreak
\begin{description}
\item[lenguas auxiliares:] cuya intención es que se emplee en la vida real como medio de comunicación común (interlingua, esperanto, o últimamente también, el inglés).
\item[lenguas ficticias:] para ser usados en obras de ficción (p. ej. el idioma \emph{klingon} usado en la saga Star Trek).
\end{description}


\section{Técnicas de preprocesamiento textual del lenguaje natural}
\label{sec:preproc-NLP}

En esta sección describimos distintas técnicas de preprocesamiento de las frases del lenguaje natural. Generalmente, la experimentación en técnicas de procesamiento de lenguaje natural se han realizado en base a \nombrebf{corpus}\index{corpus} de texto: conjuntos de ejemplo de uso real del lenguaje.

Estos corpus (véase la \autoref{tbl:corpus-list}) son grandes ficheros normalmente de texto plano con el texto en crudo, o ligeramente procesado, acompañado de métricas o etiquetas. A éstos textos, posteriormente se le puede aplicar una o más de las técnicas de preprocesamiento que se explican en esta sección, antes de pasar a la fase de entrenamiento para el componente de aprendizaje automático (machine learning); o como conjunto de datos \emph{(dataset)}\index{dataset@\emph{dataset}} para cualquier otra investigación.

El \nombrebf{Linguistic Data Consortium}\footnote{\url{https://www.ldc.upenn.edu}} (LDC\index{LDC}) es un consorcio de empresas y universidades que realiza publicaciones de grandes datasets lingüísticos, tanto en texto como en audio, mantenido por la Universidad de Pennsylvania y financiado entre otros, por la Agencia de Investigación Avanzada del Departamento de Defensa americano (\emph{Defense Advanced Research Projects Agency}, DARPA\index{DARPA}).

\begin{table}[htbp]
\centering
\begin{tabular}{|p{3.2cm}|p{12cm}|}
\hline
\nombrebf{Enron Corpus} & {\small\url{https://www.cs.cmu.edu/~./enron/}} \\ \hline
descripción: & Conjunto de 600.000 mensajes de email de los empleados de la extinguida compañía Enron~Corporation. \\ \hline
propósito: & Entrenar clasificadores de spam-ham; y por temáticas. \\ \hhline{==}
%
\nombrebf{Google's n-gram} & {\small\url{http://googleresearch.blogspot.com.es/2006/08/all-our-n-gram-are-belong-to-you.html}} \\ \hline
descripción: & 1,2,3,4 y 5-gramas de las páginas indexadas por Google con su número de apariciones. 24~GiB de texto en gzip. \\ \hline
propósito: & Conocimiento sobre la lingüística del idioma inglés por técnicas probabilísticas.\\ \hhline{==}
%
\nombrebf{Google's Wikilinks} & {\small\url{http://googleresearch.blogspot.co.uk/2013/03/learning-from-big-data-40-million.html}} \\ \hline
descripción: & Páginas enlazadas a conceptos definidos en la Wikipedia, con su contexto. \\ \hline
propósito: & Conocimiento de conceptos, web semántica, taxonomías, ontologías\ldots \\ \hhline{==}
%
\nombrebf{Treebank-3} & {\small\url{https://catalog.ldc.upenn.edu/LDC99T42}} \\ \hline
descripción: & 2.499 artículos del Wall Street Journal con anotaciones sintácticas. \\ \hline
propósito: & Conocimiento sobre la estructura gramatical del idioma inglés. \\ \hhline{==}
%
\nombrebf{TIMIT LDC93S1} & {\small\url{https://catalog.ldc.upenn.edu/LDC93S1}} \\ \hline
descripción: & Muestras de fonemas, transcripciones y pronunciación de palabras en inglés americano.\\ \hline
propósito: & Aplicaciones de reconocimiento de voz. \\ \hhline{==}
%
\nombrebf{Wiktionary RDF extraction} & {\small\url{http://wiki.dbpedia.org/wiktionary-rdf-extraction}} \\ \hline
descripción: & Metadatos RDF de las definiciones de los términos de las ediciones en distintos idiomas de Wiktionary.\\ \hline
propósito: & Web semántica, ontologías, tesauros. \\ \hline
\end{tabular}
\caption{Algunos corpus lingüísticos (datasets) existentes}
\label{tbl:corpus-list}
\end{table}

\FloatBarrier
\subsection{\emph{``Tokenización''}}

La \nombrebf{tokenización}\index{tokenización}\index{tokenization@\emph{tokenization}} en NLP es normalmente el proceso de separar la cadena de entrada, frases en lenguaje natural, en las distintas palabras que componen la frase. Puede ser simplemente un separador que trocee la frase cada vez que se encuentre un espacio.
\begin{listing}[H]
\begin{minted}{python}
>>> 'of escapades demonstrating the adage that what is good for the goose'.split()
['of', 'escapades', 'demonstrating', 'the', 'adage', 'that', 'what', 'is', 'good', 'for', 'the', 'goose']
>>> 
\end{minted}
\caption{Tokenización sencilla mediante la separación de espacios.}
\label{lst:tokenizacion-sencilla}
\end{listing}

La tokenización en su sentido más amplio y general es el procedimiento del análisis léxico para reconocer lexemas del flujo de entrada con la que se obtiene un flujo de salida compuesto por tokens \citep[§2.2]{Jimenez2004}.

\begin{definition}[Token] \index{token}
Denominamos \emph{token} al conjunto de cadenas de caracteres con significado mínimo.
\end{definition}

\begin{definition}[Patrón] \index{patrón}
El \emph{patrón} es una regla que describe el conjunto de caracteres que cuadran un token.
\end{definition}

El concepto de ``cuadrar'' se denomina en inglés como \emph{match}\index{match@\emph{match}}, y también se puede traducir como concordancia o correspondencia.

\begin{definition}[Lexema] \index{lexema}
Un \emph{lexema} es una secuencia concreta de caracteres que se corresponde con el patrón de un token determinado.
\end{definition}

\begin{example} \index{token} \index{patrón} \index{lexema}
Pongamos algunos ejemplos de tokens, patrones y lexemas, de un lenguaje formal típico de programación:
\nopagebreak
\begin{center}
\begin{tabular}{|l|l|l|}
\hline
\textbf{token} & \textbf{lexemas de ejemplo} & \textbf{descripción informal del patrón} \\ \hhline{===}
\codep{CONST} & \codep{const} & const \\ \hline
\codep{IF} & \codep{if} & if \\ \hline
\codep{OPREL} & \codep{<}, \codep{<=}, \codep{=}, \codep{>}, \codep{>=} & <~ ó <= ó = ó >~ ó >= \\ \hline
\codep{ID} & \codep{pi}, \codep{contador}, \codep{D2} & letra seguida de letras y dígitos \\ \hline
\codep{CTENUM} & \codep{3.1416}, \codep{0}, \codep{6.1E23} & cualquier constante numérica \\ \hline
\codep{LITERAL} & \codep{"core dumped"} & cualesquiera letras entre \codep{"} y \codep{"} excepto \codep{"} \\ \hline
\end{tabular}
\end{center}
\end{example}

En lenguaje natural, la unidad mínima de información textual será la palabra. De manera que token y lexema coinciden siempre, y no tiene sentido definir un patrón para cada token de las palabras de un diccionario de la lengua, ya que el patrón es la propia secuencia de letras de la palabra.

Existen distintos niveles de tokenización de documentos \citep[ch.~1]{Perkins2010}:
\nopagebreak
\begin{itemize}
\item Tokenizar textos en frases.
\item Tokenizar frases en palabras.
\end{itemize}

Simplemente para la primera tarea ya nos encontramos con dificultades, pues las frases no sólo terminan en punto. Pueden terminar en punto (\codep{.}), signo de exclamación (\codep[text]{!}), signo de interrogación (\codep[text]{?}), en algunos casos dos puntos (\codep{:}). Y en otros idiomas, como el español, también hay que tener en cuenta que en la separación entre frases puede haber signos de apertura de interrogación o exclamación, etc. Y también hay que tener en cuenta los puntos que separan unidades de millar en una cifra, o los puntos suspensivos, los puntos usados en acrónimos y abreviaturas, etc., para no interpretarlo como separador de frases en estos casos.

En cuanto a la separación de palabras, también pueden surgir problemas, dependiendo del idioma. En el siguiente ejemplo (\autoref{lst:wordtokenizers}) podemos ver distintas formas de separar palabras de una frase en inglés que usa contracciones.\footnote{También se puede encontrar un demostrador interactivo de la tokenización en la siguiente dirección:\\
\url{http://text-processing.com/demo/tokenize/}}

\begin{listing}[H]
\begin{minted}{python}
>>> from nltk.tokenize import TreebankWordTokenizer, WordPunctTokenizer, WhitespaceTokenizer
>>> TreebankWordTokenizer().tokenize("Can't is a contraction.")
['Ca', "n't", 'is', 'a', 'contraction', '.']
>>> WordPunctTokenizer().tokenize("Can't is a contraction.")
['Can', "'", 't', 'is', 'a', 'contraction', '.']
>>> WhitespaceTokenizer().tokenize("Can't is a contraction.")
["Can't", 'is', 'a', 'contraction.']
>>> 
\end{minted}
\caption{Diferentes estrategias de separación de palabras}
\label{lst:wordtokenizers}
\end{listing}

Una mejor aproximación al problema es reemplazar ---previamente a la tokenización---, las palabras contraídas con sus palabras equivalentes sin contracción.\footnote{Puede obtenerse un catálogo bastante extenso de contracciones en inglés ---en uso y arcaicas---, en la dirección:\\ \url{http://www.enchantedlearning.com/grammar/contractions/list.shtml}}
Para ello es necesario construirse una tabla que establezca el mapping \citep{Perkins2014}.

Como ejemplo de un conjunto de substitución, véase una tabla no exhaustiva en la \autoref{tbl:contracciones}, en dos versiones:
\nopagebreak
\begin{enumerate}[(a)]
\item En la primera de ellas se implementaría empleando un diccionario común de palabras de búsqueda y reemplazo, directamente.
\item En la segunda opción, se hace uso de expresiones regulares para condensar la tabla y no tener que hacer explícito todas las posibles contracciones que compartan sufijos sinónimos.
\end{enumerate}

\begin{table}[htbp]
\centering
\begin{subtable}[t]{0.45\linewidth}
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{contracción} & \textbf{reemplazar con} \\ \hhline{==}
isn't & is not \\ \hline
aren't & are not \\ \hline
wasn't & was not \\ \hline
weren't & were not \\ \hline
haven't & have not \\ \hline
hasn't & has not \\ \hline
couldn't & could not \\ \hline
shouldn't & should not \\ \hline
mightn't & might not \\ \hline
mustn't & must not \\ \hline
could've & could have \\ \hline
might've & might have \\ \hline
must've & must have \\ \hline
ma'am & madam \\ \hline
she'd've & she would have \\ \hline
'tisn't & it is not \\ \hline
who'll & who will \\ \hline
when'll & when will \\ \hline
what're & what are \\ \hline
we're & we are \\ \hline
they're & they are \\ \hline
that's & that is \\ \hline
I'll & I will \\ \hline
you'll & you will \\ \hline
he'll & he will \\ \hline
she'll & she will \\ \hline
\end{tabular}
\caption{Sustitución literal}
\label{tbl:contracciones-directa}
\end{subtable}
\begin{subtable}[t]{0.45\linewidth}
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{contracción} & \textbf{reemplazar con} \\ \hhline{==}
\verb=r"won't"= & \verb="will not"= \\ \hline
\verb=r"can't"= & \verb="cannot"= \\ \hline
\verb=r"i'm"= & \verb="i am"= \\ \hline
\verb=r"ain't"= & \verb="is not"= \\ \hline
\verb=r"(\w+)'ll"= & \verb="\g<1> will"= \\ \hline
\verb=r"(\w+)n't"= & \verb="\g<1> not"= \\ \hline
\verb=r"(\w+)'ve"= & \verb="\g<1> have"= \\ \hline
\verb=r"(\w+)'s"= & \verb="\g<1> is"= \\ \hline
\verb=r"(\w+)'re"= & \verb="\g<1> are"= \\ \hline
\verb=r"(\w+)'d"= & \verb="\g<1> would"= \\ \hline
\end{tabular}
\caption{Sustitución condensada mediante expresiones regulares}
\label{tbl:contracciones-regexp}
\end{subtable}
\vfill
\caption{Sustitución de contracciones en inglés}
\label{tbl:contracciones}
\end{table}

\FloatBarrier
\subsection{Supresión de palabras vacías \emph{(stopwords)}}

Las \nombrebf{stopwords}\index{stopword@\emph{stopword}} (o palabras vacías) son palabras que no contribuyen al significado de la frase, o al menos no son útiles para la recuperación de la información \citep{Perkins2010}. Ejemplos: «the», «thence», «th», «vs», «thereby», «c'mon», «clearly», \ldots

Conviene usarlo en técnicas de aprendizaje probabilístico, pues reduce el tamaño del vocabulario a tener en cuenta y, consecuentemente, de los índices o bases de conocimiento que se puedan generar.

En el \autoref{lst:stopwords} se puede ver un ejemplo de cómo eliminar stopwords usando el conjunto de stopwords que viene con NLTK.

\begin{listing}[htbp]
\begin{minted}{python}
>>> from nltk.corpus import stopwords
>>> words = 'the quick brown fox'.split(' ')
>>> stopwords_set = set(stopwords.words('english'))
>>> [w for w in words if w not in stopwords_set]
['quick', 'brown', 'fox']
>>> 
\end{minted}
\caption{Supresión de palabras vacías (stopwords)}
\label{lst:stopwords}
\end{listing}

\FloatBarrier
\subsection{Radicación \emph{``Stemming''}}

La \nombrebf{decodificación primaria}\index{decodificación primaria} es la herramienta instintiva que permite a los hablantes de un idioma intuir el significado de una palabra desconocida del idioma ---siendo conocido el subconjunto común y más frecuente del mismo---, a partir de sus elementos léxicos \citep{Zubira2002}. Son tres:
\nopagebreak
\begin{description}
\item[La contextualización.]\index{contextualización} Por medio de este mecanismo rastreamos el significado de la palabra valiéndonos del contexto o entorno de las frases en las cuales está inscrita. Ejemplos:
\nopagebreak
\begin{itemize}
\item ``Lo admiran por su \textbf{sindéresis}, ya que su \emph{capacidad para juzgar} es notable.'' $\Longrightarrow$ El contexto nos permite deducir que el significado de sindéresis es la capacidad para juzgar.
\item ``Los \emph{derechos económicos, sociales y culturales} son de segundo orden. Los \textbf{DESC} suponen mayores demandas.'' $\Longrightarrow$ El contexto nos permite intuir que el acrónimo DESC se refiere a derechos económicos, sociales y culturales.
\end{itemize}
%
\item[La sinonimia.]\index{sinonimia} Mediante la sinonimia, se puede hacer coincidir el significado de la palabra desconocida con términos semejantes que se han mencionado antes de ella (anáfora) o que se mencionarán después (catáfora) en el texto. Ejemplo:
\nopagebreak
\begin{itemize}
\item ``Nadie ha visto al \textbf{abate}. Parece que este \emph{clérigo} ha desaparecido.'' $\Longrightarrow$ El sinónimo de abate es clérigo, aunque seguramente tenga algún matiz que lo convierte en un término con mayor especificidad.
\end{itemize}
%
\item[La radicación.]\index{radicación} En inglés \emph{stemming}\index{stemming@\emph{stemming}}. La radicación consiste en descomponer la palabra en sus elementos constituyentes o afijos: prefijos, infijos y sufijos. Ejemplos:\footnote{Pueden consultarse más ejemplos en \url{http://www.slideshare.net/sandracasierra/decodificacin-primaria}}
\nopagebreak
\begin{itemize}
\item \textbf{postdiluviano} $\Longrightarrow$ Después (post-) de un diluvio.
\item \textbf{subtropical}  $\Longrightarrow$ Por debajo (sub-) del trópico.
\end{itemize}
\end{description}

La manera más común de realizar la radicación\index{radicación}\index{stemming@\emph{stemming}} en aplicaciones de NLP es eliminar los sufijos para quedarse con la raíz de la palabra, y su significado más primitivo (véase \autoref{lst:PorterStemmer}).

\begin{listing}[htbp]
\begin{minted}{python}
>>> from nltk.stem import PorterStemmer
>>> stemmer = PorterStemmer()
>>> stemmer.stem('cookery')
'cookeri'
>>> stemmer.stem('cooking')
'cook'
>>> stemmer.stem('series')
'seri'
>>> stemmer.stem('women')
'women'
>>> stemmer.stem('riding')
'ride'
>>> 
\end{minted}
\caption{Funcionamiento de \codep{PorterStemmer}}
\label{lst:PorterStemmer}
\end{listing}

\codep{PorterStemmer} es la implementación en NLTK del algoritmo de radicación de Martin Porter \citep{Porter1980}. Es un algoritmo ---muy usado para lengua inglesa---, de eliminación o reemplazo de sufijos en varias iteraciones, bastante eficiente.

Aunque se pierda algo de información, tiene sus ventajas:
\nopagebreak
\begin{itemize}
\item Sirve para reducir el tamaño del vocabulario; y por consiguiente, el orden del tiempo y el espacio que ocupan los programas de procesamiento.
\item Sirve para asemejar conceptos similares, que es normalmente lo deseable al hacer una búsqueda o extracción de información.
\item También sirve para acelerar el procesamiento y reducir el tamaño de los índices de los motores de búsqueda.
\end{itemize}

No obstante también tiene alguna desventaja:
\nopagebreak
\begin{itemize}
\item Al estar basado en la eliminación literal de sufijos, puede producir palabras no válidas del idioma (como en \codep{'cookeri'} en el ejemplo anterior).
\end{itemize}

NLTK proporciona otros stemmers adicionales a \codep{PorterStemmer}:
\begin{description}
\item[\codep{LancasterStemmer}] Desarrollado en la Universidad de Lancaster, es ligeramente más agresivo que el algoritmo de Porter, aunque no existen estudios que confirmen la supremacía de uno sobre el otro \citep{Perkins2014}.
\item[\codep{RegexpStemmer}] Stemmer al que se puede recurrir para eliminar prefijos o sufijos de manera personalizada configurado mediante una expresión regular.
\item[\codep{SnowballStemmer}] Snowball extiende el método Porter para su aplicación a otros idiomas indoeuropeos: románicos, germánicos y escandinavos.
\end{description}

\FloatBarrier
\subsection{Lematización}

La \nombrebf{lematización}\index{lematización} (\emph{lemmatizing}\index{lemmatizing@\emph{lemmatizing}} en inglés) es el proceso lingüístico que sustituye una palabra con forma \nombrebf{flexionada} (plural, femenino, verbo conjugado, etc.) por su \nombrebf{lema}. Tiene el mismo objetivo que la radicación, pero a diferencia de aquel, la lematización siempre produce una palabra válida en el idioma.

\begin{definition}[Lema]
En lingüística, el \nombrebf{lema}\index{lema} es la forma que por convenio se acepta como representante de todas las formas flexionadas de una misma palabra.
\end{definition}

En el \autoref{lst:WordNetLemmatizer} se muestra un ejemplo de lematización usando WordNet en NLTK. La función en bastante conservadora; si no sabe qué hacer, deja la palabra como está. En el ejemplo se muestra cómo intervenir en el reemplazo del sinónimo si le indicamos cuál es la parte de la oración (part-of-speech) ---un verbo en este caso.

\begin{listing}[htbp]
\begin{minted}{python}
>>> from nltk.stem import WordNetLemmatizer
>>> lemmatizer = WordNetLemmatizer()
>>> lemmatizer.lemmatize('cooking')
'cooking'
>>> lemmatizer.lemmatize('cooking', pos='v')
'cook'
>>> lemmatizer.lemmatize('cookbooks')
'cookbook'
>>> 
\end{minted}
\caption{Lematización en NLTK}
\label{lst:WordNetLemmatizer}
\end{listing}


\FloatBarrier
\subsection{Anotación gramatical de la parte de la oración \emph{(Part-of-speech tagging)}}

En gramática tradicional, la clasificación según categorías gramaticales es de tipo semántico y no funcional. En la gramática española el término fue introducido por Antonio de Nebrija \citep[Categoría gramatical]{wikipedia-es}, aunque en otros idiomas se pueden establecer diferentes categorías gramaticales. La gramática tradicional distingue nueve \nombrebf{partes de la oración}\index{parte de la oración}\index{part-of-speech@\emph{part-of-speech}} (las ocho de Nebrija más el artículo):
\nopagebreak
\begin{enumerate*}
\item Artículo o determinante
\item Sustantivo o nombre
\item Pronombre
\item Verbo
\item Adjetivo
\item Adverbio
\item Preposición
\item Conjunción
\item Interjección
\end{enumerate*}

El proceso de anotación gramatical POS convierte la lista de palabras, en una lista de pares de palabras con su etiqueta POS indicando si funciona como nombre, verbo, adjetivo, \ldots

En NLTK existen diversos \emph{taggers}\index{POS tagger@\emph{POS tagger}}, y pueden combinarse en una cadena de taggers de manera que si uno no es capaz de asignar la etiqueta sin ambigüedad, se intenta con el siguiente en la cadena \citep{Perkins2010}.

En los taggers se pueden establecer reglas que permitan identificar la parte de la oración de la palabra a partir de su lexicografía, o bien teniendo en cuenta el contexto (un número de palabras anteriores). Por ejemplo: si sólo vemos la palabra \codep{'la'}, podemos suponer que se trata del artículo determinado femenino singular, que es lo más común; mientras que si tenemos en cuenta la palabra en su contexto: \codep{'la sigue a sol en la escala de notas musicales'}, la palabra \codep{'la'} funciona como sustantivo al ser el nombre de una nota musical.

Para estos casos, el tagger dependiente del contexto se entrena mediante alguna técnica de aprendizaje automático sobre un corpus previamente anotado manualmente. De este entrenamiento, el tagger extrae la reglas de prelación, o realiza la inferencia estadística que le permita identificarlo correctamente para la mayoría de los casos. Evidentemente, funcionará mejor cuando el texto a anotar sea del mismo dominio del lenguaje que el texto usado para el entrenamiento.

También existen distintos conjuntos de etiquetas posibles, dependiendo del autor y del idioma a anotar. En la \autoref{tbl:pos-tags} se muestran (a) los que se utilizan en el proyecto Penn Treebank de la Universidad de Pennsylvania,\footnote{Extraído de \url{http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html}} (b) la correspondencia con los tags del corpus de WordNet.

\begin{table}[htbp]
\centering
\begin{subtable}[t]{0.60\linewidth}
\centering
\begin{tabular}{|l|l|l|}
\hline
\textbf{\#} & \textbf{tag} & \textbf{descripción} \\ \hhline{===}
1 & \verb=CC= & Coordinating conjunction \\ \hline
2 & \verb=CD= & Cardinal number \\ \hline
3 & \verb=DT= & Determiner \\ \hline
4 & \verb=EX= & Existential \emph{there} \\ \hline
5 & \verb=FW= & Foreign word \\ \hline
6 & \verb=IN= & Preposition or subordinating conjunction \\ \hline
7 & \verb=JJ= & Adjective \\ \hline
8 & \verb=JJR= & Adjective, comparative \\ \hline
9 & \verb=JJS= & Adjective, superlative \\ \hline
10 & \verb=LS= & List item marker \\ \hline
11 & \verb=MD= & Modal \\ \hline
12 & \verb=NN= & Noun, singular or mass \\ \hline
13 & \verb=NNS= & Noun, plural \\ \hline
14 & \verb=NNP= & Proper noun, singular \\ \hline
15 & \verb=NNPS= & Proper noun, plural \\ \hline
16 & \verb=PDT= & Predeterminer \\ \hline
17 & \verb=POS= & Possessive ending \\ \hline
18 & \verb=PRP= & Personal pronoun \\ \hline
19 & \verb=PRP$= & Possessive pronoun \\ \hline
20 & \verb=RB= & Adverb \\ \hline
21 & \verb=RBR= & Adverb, comparative \\ \hline
22 & \verb=RBS= & Adverb, superlative \\ \hline
23 & \verb=RP= & Particle \\ \hline
24 & \verb=SYM= & Symbol \\ \hline
25 & \verb=TO= & \emph{to} \\ \hline
26 & \verb=UH= & Interjection \\ \hline
27 & \verb=VB= & Verb, base form \\ \hline
28 & \verb=VBD= & Verb, past tense \\ \hline
29 & \verb=VBG= & Verb, gerund or present participle \\ \hline
30 & \verb=VBN= & Verb, past participle \\ \hline
31 & \verb=VBP= & Verb, non-3rd person singular present \\ \hline
32 & \verb=VBZ= & Verb, 3rd person singular present \\ \hline
33 & \verb=WDT= & Wh-determiner \\ \hline
34 & \verb=WP= & Wh-pronoun \\ \hline
35 & \verb=WP$= & Possessive wh-pronoun \\ \hline
36 & \verb=WRB= & Wh-adverb \\ \hline
\end{tabular}
\caption{POS tags del Penn Treebank}
\label{tbl:pos-tags-peen-treebank}
\end{subtable}
\begin{subtable}{0.38\linewidth}
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{WordNet} & \textbf{Treebank} \\ \hhline{==}
n & NN \\ \hline
a & JJ \\ \hline
s & JJ \\ \hline
r & RB \\ \hline
v & VB \\ \hline
\end{tabular}
\caption{POS tags correspondientes en WordNet}
\label{tbl:pos-tags-wordnet}
\end{subtable}
\caption{Etiquetas POS en Penn Treebank y en WordNet}
\label{tbl:pos-tags}
\end{table}

\FloatBarrier
\subsection{Resumen de procedimientos de preprocesamiento}

\begin{figure}[H]
\includegraphics[width=0.98\textwidth]{procesamiento-textos-mod-ir}
\caption[Resumen de procedimientos de preprocesamiento]{Resumen de procedimientos de preprocesamiento \citep[ch. 7]{Baeza-Yates2011}}
\end{figure}

\section{Análisis sintáctico}

El \nombrebf{análisis sintáctico}\index{análisis!sintáctico} \emph{(parsing)}\index{parsing@\emph{parsing}} de una oración se realiza en dos pasos:
\begin{enumerate}
\item Realizar la anotación de la categoría gramatical o parte de la oración (POS tagging) de cada palabra de la oración. En caso de ambigüedades, mantenerlas todas, o bien la más probable.
\item Calcular uno o varios de los árboles de derivación teniendo en cuenta las reglas de construcción (reglas de producción) de estructuras sintácticas de la gramática del idioma.
\end{enumerate}

En la \autoref{fig:arbol-sintactico} se muestra un ejemplo de árbol sintáctico anotado con la categoría gramatical.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.7\textwidth]{arbol-sintactico}
\caption{Arbol sintáctico}
\label{fig:arbol-sintactico}
\end{figure}

\section{Modelo de espacio vectorial semántico}\label{sec:MEVS}

Se han usado \nombrebf{modelos de espacios vectoriales} (MEV)\index{MEV} para modelar la semántica de las palabras. A cada palabra del vocabulario se le asigna un vector $n$-dimensional, y es un hecho que este vector puede representar de alguna manera el significado de la palabra \citep{Socher2012}.

Para posicionar cada palabra dentro del MEV, el sistema se entrena a partir de grandes corpus de texto continuo no estructurado. Se toma cada palabra con su contexto ($n$-gramas), y se relacionará la misma palabra con los futuros contextos en los que aparezca.

Existen varios métodos para realizar este entrenamiento no supervisado:
\begin{description}
\item[continous bag-of-words] (CBOW)\index{CBOW} Se considera la palabra y un contexto de $n-1$ palabras posteriores consecutivas, que se tratará como un conjunto (saco de palabras), sin tener en cuenta su orden de aparición en la frase.
\item[skip-gram] \index{skip-gram@\emph{skip-gram}} En este modelo, se considera la palabra y un contexto de $n-1$ palabras a elegir de entre las $n-1 + k$ palabras posteriores consecutivas; «saltando» de esta manera hasta un máximo de $k$ palabras, teniendo además en cuenta su orden de aparición.
\end{description}

\begin{example}
Sea la frase «the rain in Spain falls mainly on the plain»:
\begin{itemize}
\item $\text{CBOW}(n=2)$: se toman las parejas $(n=2)$ de palabras consecutivas:
$\{\text{the}, \text{rain}\}$,
$\{\text{rain}, \text{in}\}$,
$\{\text{in}, \text{Spain}\}$,
$\{\text{Spain}, \text{falls}\}$, \ldots
\item $\text{skip-gram}(n=2,k=1)$: se toman las parejas $(n=2)$ de palabras consecutivas, y además las parejas de palabras con distancia de $k=1$ palabra: 
$[\text{the}, \text{rain}]$,
$[\text{the}, \text{in}]$,
$[\text{rain}, \text{in}]$,
$[\text{rain}, \text{Spain}]$, 
$[\text{in}, \text{Spain}]$, 
$[\text{in}, \text{falls}]$, 
$[\text{Spain}, \text{falls}]$, 
$[\text{Spain}, \text{mainly}]$, \ldots
\end{itemize}
\end{example}

En el modelo de espacio vectorial, es importante tener una medida de la similitud entre conceptos, para aplicaciones de recuperación de la información.
\begin{definition}[Similitud coseno]
La \nombrebf{similitud coseno} (o distancia coseno) entre dos vectores $\vec{A}$ y $\vec{B}$ en un modelo de espacio vectorial se define como la relación entre el producto escalar de ambos y el producto de sus módulos:
\begin{eqnarray}
\cos(\theta) &=& \frac{\vec{A}\cdot\vec{B}}{\|\vec{A}\| \|\vec{B}\|}
\end{eqnarray}
\end{definition}

De esta manera, dos vectores estarán separados según la distancia coseno en un valor en el rango $\cos(\theta) \in [-1,1]$, con la siguiente interpretación:
\begin{itemize}
\item Dos vectores con gran similitud (conceptos relacionados, palabras sinónimas, etc.) que apunten al mismo sitio, formarán un ángulo $\theta$ próximo a~0º, y por tanto un coseno próximo a 1.
\item Dos vectores con mínima similitud, que sean prácticamente ortogonales (conceptos o palabras sin relación), con ángulo $\theta \approx 90^{\circ}$ tendrá un coseno próximo a 0.
\item Dos vectores con gran similitud y sentidos opuestos, (conceptos contrarios, palabras antónimas, etc.), tendrán un coseno próximo a -1.
\end{itemize}

Después de realizar un entrenamiento en un gran corpus ---y usando un espacio vectorial de grandes dimensiones---, cuando el modelo converge, se observa que palabras con significado similar les corresponden vectores cercanos (en términos de la distancia coseno entre ellos). Por ejemplo, «powerful» y «strong» están cercanos, mientras que «powerful» y «Paris» no lo están. Incluso las operaciones algebraicas entre vectores semánticos también incorporan significado, de manera que puede responder a ciertas cuestiones por analogía \citep{DBLP:journals/corr/LeM14}:
\[
\text{«king»} - \text{«man»} + \text{«woman»} \approx \text{«queen»}
\]

Se pueden ver más ejemplos de relaciones análogas aprendidas mediante este sistema en la \autoref{tbl:examples-skip-gram}.

\begin{table}[htbp]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\nombrebf{relación} & \nombrebf{ejemplo 1} & \nombrebf{ejemplo 2} & \nombrebf{ejemplo 3} \\ \hhline{====}
France - Paris & Italy: Rome  & Japan: Tokyo & Florida: Tallahassee \\ \hline
big - bigger & small: larger & cold: colder & quick: quicker \\ \hline
Miami - Florida & Baltimore: Maryland & Dallas: Texas & Kona: Hawaii \\ \hline
Einstein - scientist &  Messi: midfielder  & Mozart: violinist & Picasso: painter \\ \hline
Sarkozy - France &  Berlusconi: Italy  & Merkel: Germany & Koizumi: Japan \\ \hline
copper - Cu &  zinc: Zn & gold: Au & uranium: plutonium \\ \hline
Berlusconi - Silvio & Sarkozy: Nicolas  & Putin: Medvedev & Obama: Barack \\ \hline
Microsoft - Windows  & Google: Android  & IBM: Linux & Apple: iPhone \\ \hline
Microsoft - Ballmer  & Google: Yahoo & IBM: McNealy & Apple: Jobs \\ \hline
Japan - sushi & Germany: bratwurst & France: tapas & USA: pizza \\ \hline
\end{tabular}
\caption[Ejemplos de las relaciones aprendidas entre parejas de palabras (modelo \emph{skip-gram})]{Ejemplos de las relaciones aprendidas entre parejas de palabras. Se ha empleado un modelo skip-gram entrenado con 783 mill. de palabras y 300 dimensiones, \citep{Mikolov2013}.}
\label{tbl:examples-skip-gram}
\end{table}

