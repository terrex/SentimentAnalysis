Automatically generated by Mendeley Desktop 1.14
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@book{Leiva2012,
author = {Leiva-Aguilera, J},
isbn = {9788497889902},
publisher = {UOC},
series = {El profesional de la informaci\'{o}n},
title = {{Gesti\'{o}n de la reputaci\'{o}n online}},
year = {2012}
}
@misc{Gabilondo,
author = {Gabilondo, Michael},
file = {:Users/terrex/Documents/Mendeley Desktop/Gabilondo/Unknown/Gabilondo - Unknown - How to perform some common NLP tasks using NLTK.pdf:pdf},
keywords = {Gabilondo2011},
mendeley-tags = {Gabilondo2011},
title = {{How to perform some common NLP tasks using NLTK}},
url = {http://www.cs.ucf.edu/courses/cap5636/fall2011/nltk.pdf},
urldate = {2014-11-09}
}
@misc{Perkins,
author = {Perkins, Jacob},
keywords = {streamhacker},
mendeley-tags = {streamhacker},
title = {{Python NLTK Natural Language Processing | StreamHacker}},
url = {http://streamhacker.com/},
urldate = {2014-11-09}
}
@book{Hearst2009,
abstract = {This book focuses on the human users of search engines and the tool they use to interact with them: the search user interface. The truly worldwide reach of the Web has brought with it a new realization among computer scientists and laypeople of the enormous importance of usability and user interface design. In the last ten years, much has become understood about what works in search interfaces from a usability perspective, and what does not. Researchers and practitioners have developed a wide range of innovative interface ideas, but only the most broadly acceptable make their way into major web search engines. This book summarizes these developments, presenting the state of the art of search interface design, both in academic research and in deployment in commercial systems. Many books describe the algorithms behind search engines and information retrieval systems, but the unique focus of this book is specifically on the user interface. It will be welcomed by industry professionals who design systems that use search interfaces as well as graduate students and academic researchers who investigate information systems.},
annote = {http://people.ischool.berkeley.edu/\~{}hearst/talks/upitt.pdf},
author = {Hearst, Marti A.},
booktitle = {Search User Interfaces},
doi = {10.1145/2018396.2018414},
isbn = {0521113792},
issn = {00010782},
keywords = {among others,at,face,inter-,is promoted by researchers,microsoft,natural inter-,not sur-,people are drawn to,prisingly,the term},
number = {Ch 1},
pages = {404},
pmid = {15784401},
publisher = {Search User Interfaces},
title = {{Search User Interfaces}},
url = {http://searchuserinterfaces.com/book/},
volume = {54},
year = {2009}
}
@book{Norvig2014,
abstract = {Paradigms of AI Programming is the first text to teach advanced Common Lisp techniques in the context of building major AI systems. By reconstructing authentic, complex AI programs using state-of-the-art Common Lisp, the book teaches students and professionals how to build and debug robust practical programs, while demonstrating superior programming style and important AI concepts. The author strongly emphasizes the practical performance issues involved in writing real working programs of significant size. Chapters on troubleshooting and efficiency are included, along with a discussion of the fundamentals of object-oriented programming and a description of the main CLOS functions. This volume is an excellent text for a course on AI programming, a useful supplement for general AI courses and an indispensable reference for the professional programmer.},
author = {Norvig, Peter},
isbn = {0080571158},
pages = {946},
publisher = {Elsevier Science},
title = {{Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp}},
url = {https://books.google.com/books?id=eH6jBQAAQBAJ\&pgis=1},
year = {2014}
}
@book{Perkins2010,
abstract = {Text Processing in Python describes techniques for manipulation of text using the Python programming language. At the broadest level, text processing is simply taking textual information and doing something with it. This might be restructuring or reformatting it, extracting smaller bits of information from it, or performing calculations that depend on the text. Text processing is arguably what most programmers spend most of their time doing. Because Python is clear, expressive, and object-oriented it is a perfect language for doing text processing, even better than Perl. As the amount of data everywhere continues to increase, this is more and more of a challenge for programmers. This book is not a tutorial on Python. It has two other goals: helping the programmer get the job done pragmatically and efficiently; and giving the reader an understanding - both theoretically and conceptually - of why what works and what doesnt work doesnt work. Mertz provides practical pointers and tips that emphasize efficient, flexible, and maintainable approaches to the text processing tasks that working programmers face daily.},
author = {Perkins, Jacob},
booktitle = {Python},
file = {:Users/terrex/Documents/Mendeley Desktop/Perkins/2010/Perkins - 2010 - Python Text Processing with NLTK 2.0 Cookbook.pdf:pdf},
isbn = {9781849513609},
pages = {544},
publisher = {Packt Publishing Ltd},
title = {{Python Text Processing with NLTK 2.0 Cookbook}},
year = {2010}
}
@article{Turney2010,
abstract = {Computers understand very little of the meaning of human language. This profoundly limits our ability to give instructions to computers, the ability of computers to explain their actions to us, and the ability of computers to analyse and process text. Vector space models (VSMs) of semantics are beginning to address these limits. This paper surveys the use of VSMs for semantic processing of text. We organize the literature on VSMs according to the structure of the matrix in a VSM. There are currently three broad classes of VSMs, based on term-document, word-context, and pair-pattern matrices, yielding three classes of applications. We survey a broad range of applications in these three categories and we take a detailed look at a specific open source project in each category. Our goal in this survey is to show the breadth of applications of VSMs for semantics, to provide a new perspective on VSMs for those who are already familiar with the area, and to provide pointers into the literature for those who are less familiar with the field.},
address = {USC INFORMATION SCIENCES INST, 4676 ADMIRALITY WAY, MARINA DEL REY, CA 90292-6695 USA},
author = {Turney, Peter D. and Pantel, Patrick},
file = {:Users/terrex/Documents/Mendeley Desktop/Turney, Pantel/2010/Turney, Pantel - 2010 - From frequency to meaning Vector space models of semantics.pdf:pdf},
issn = {1076-9757},
journal = {Journal of Artificial Intelligence Research},
keywords = {ALGORITHM,CATEGORIZATION,COMMUNICATION,COOCCURRENCE,DECOMPOSITION,INFORMATION,MATHEMATICAL-THEORY,RETRIEVAL,SIMILARITY,TEXT},
pages = {141--188},
publisher = {AI ACCESS FOUNDATION},
title = {{From frequency to meaning: Vector space models of semantics}},
type = {Article},
volume = {37},
year = {2010}
}
@misc{Shogun2015,
author = {{The Shogun Toolbox Foundation}},
file = {:Users/terrex/Documents/Mendeley Desktop/The Shogun Toolbox Foundation/2015/The Shogun Toolbox Foundation - 2015 - Machine Learning toolboxes feature comparision matrix.pdf:pdf},
title = {{Machine Learning toolboxes feature comparision matrix}},
url = {http://www.shogun-toolbox.org/page/features/},
urldate = {2015-06-17},
year = {2015}
}
@inproceedings{Mikolov2011,
abstract = {We present several modifications of the original recurrent neural net work language model (RNN LM). While this model has been shown to significantly outperform many competitive language modeling techniques in terms of accuracy, the remaining problem is the computational complexity. In this work, we show approaches that lead to more than 15 times speedup for both training and testing phases. Next, we show importance of using a backpropagation through time algorithm. An empirical comparison with feedforward networks is also provided. In the end, we discuss possibilities how to reduce the amount of parameters in the model. The resulting RNN model can thus be smaller, faster both during training and testing, and more accurate than the basic one.},
author = {Mikolov, Tomas and Kombrink, Stefan and Burget, Lukas and Cernocky, Jan and Khudanpur, Sanjeev},
booktitle = {2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP.2011.5947611},
file = {:Users/terrex/Documents/Mendeley Desktop/Mikolov et al/2011/Mikolov et al. - 2011 - Extensions of recurrent neural network language model.pdf:pdf},
isbn = {978-1-4577-0538-0},
issn = {1520-6149},
keywords = {Artificial neural networks,Backpropagation,Computational modeling,Probability distribution,Recurrent neural networks,Training,Vocabulary,backpropagation,competitive language modeling techniques,computational complexity,feedforward network,feedforward neural nets,language modeling,natural language processing,recurrent neural nets,recurrent neural network language model,recurrent neural networks,speech recognition},
month = may,
pages = {5528--5531},
publisher = {IEEE},
shorttitle = {Acoustics, Speech and Signal Processing (ICASSP)},
title = {{Extensions of recurrent neural network language model}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5947611},
year = {2011}
}
@misc{Hilbert2012,
abstract = {This is Part I of a two-part article that reviews methodological and statistical challenges involved in the estimation of humanity’s technological capacity to communicate, store, and compute information. It is written from the perspective of the results of our recent inventory of 60 technological categories between 1986 and 2007 (measured in bits and MIPS [million-instructions-per-second]). In Part I, we summarize the results of our inventory, and explore a series of basic choices that must be made in the course of measuring information and communication capacities. The most basic underlying assumptions behind our estimates include—among others—decisions about what is counted as (1) communication, (2) storage, and (3) computation; if technological capacities or consumption of information is measured; and if unique information is distinguished from duplicate information. We compare our methodological choices with different approaches taken in similar studies. The article shows how the particular question on the researcher’s mind, as well as the availability of source data has and will influence most of the methodological choices in different exercises.},
author = {Hilbert, Martin and L\'{o}pez, Priscila},
booktitle = {International Journal of Communication},
file = {:Users/terrex/Documents/Mendeley Desktop/Hilbert, L\'{o}pez/2012/Hilbert, L\'{o}pez - 2012 - How to measure the world's technological capacity to communicate, store, and compute information, part I Result.pdf:pdf},
isbn = {1932-8036},
issn = {19328036},
number = {1},
pages = {956--979},
title = {{How to measure the world's technological capacity to communicate, store, and compute information, part I: Results and scope}},
volume = {6},
year = {2012}
}
@inproceedings{Pang2005,
abstract = {We address the rating-inference problem, wherein rather than simply decide whether a review is "thumbs up" or "thumbs down", as in previous sentiment analysis work, one must determine an author's evaluation with respect to a multi-point scale (e.g., one to five "stars"). This task represents an interesting twist on standard multi-class text categorization because there are several different degrees of similarity between class labels; for example, "three stars" is intuitively closer to "four stars" than to "one star". We first evaluate human performance at the task. Then, we apply a meta-algorithm, based on a metric labeling formulation of the problem, that alters a given n-ary classifier's output in an explicit attempt to ensure that similar items receive similar labels. We show that the meta-algorithm can provide significant improvements over both multi-class and regression versions of SVMs when we employ a novel similarity measure appropriate to the problem.},
annote = {http://www.aclweb.org/anthology/P05-1015},
archivePrefix = {arXiv},
arxivId = {cs/0506075},
author = {Pang, Bo and Lee, Lillian},
booktitle = {Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics},
doi = {10.3115/1219840.1219855},
eprint = {0506075},
file = {:Users/terrex/Documents/Mendeley Desktop/Pang, Lee/2005/Pang, Lee - 2005 - Seeing stars Exploiting class relationships for sentiment categorization with respect to rating scales.pdf:pdf},
isbn = {1932432515},
keywords = {Pang2005},
mendeley-tags = {Pang2005},
month = jun,
pages = {115--124},
primaryClass = {cs},
publisher = {Association for Computational Linguistics},
series = {ACL'05},
title = {{Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales}},
year = {2005}
}
@misc{TheStanfordNaturalLanguageProcessingGroup,
author = {{The Stanford Natural Language Processing Group}},
keywords = {standforparser},
mendeley-tags = {standforparser},
title = {{The Stanford Parser: A statistical parser}},
url = {http://nlp.stanford.edu/software/lex-parser.shtml},
urldate = {2014-09-11}
}
@inproceedings{Mikolov2013,
abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3781v3},
author = {Mikolov, Tomas and Corrado, Greg and Chen, Kai and Dean, Jeffrey},
booktitle = {Proceedings of the International Conference on Learning Representations (ICLR 2013)},
eprint = {arXiv:1301.3781v3},
file = {:Users/terrex/Documents/Mendeley Desktop/Mikolov et al/2013/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Vector Space.pdf:pdf},
pages = {1--12},
title = {{Efficient Estimation of Word Representations in Vector Space}},
year = {2013}
}
@misc{RuizReina2013,
author = {{Ruiz Reina}, J. L.},
file = {:Users/terrex/Documents/Mendeley Desktop/Ruiz Reina/2013/Ruiz Reina - 2013 - Aprendizaje de modelos probabil\'{\i}sticos.pdf:pdf},
publisher = {Dpto. Ciencias de la Computaci\'{o}n e Inteligencia Artificial. Universidad de Sevilla},
title = {{Aprendizaje de modelos probabil\'{\i}sticos}},
url = {http://www.cs.us.es/cursos/ia2/temas/tema-04.pdf},
urldate = {2014-11-09},
year = {2013}
}
@article{Jansen2009,
author = {Jansen, Bernard J and Zhang, Mimi and Sobel, Kate and Chowdury, Abdur},
file = {:Users/terrex/Documents/Mendeley Desktop/Jansen et al/2009/Jansen et al. - 2009 - Twitter power Tweets as electronic word of mouth.pdf:pdf},
journal = {Journal of the American society for information science and technology},
number = {11},
pages = {2169--2188},
publisher = {Wiley Online Library},
title = {{Twitter power: Tweets as electronic word of mouth}},
volume = {60},
year = {2009}
}
@inproceedings{Socher2012,
abstract = {Single-word vector space models have been very successful at learning lexical informa- tion. However, they cannot capture the com- positional meaning of longer phrases, prevent- ing them from a deeper understanding of lan- guage. We introduce a recursive neural net- work (RNN) model that learns compositional vector representations for phrases and sen- tences of arbitrary syntactic type and length. Our model assigns a vector and a matrix to ev- ery node in a parse tree: the vector captures the inherent meaning of the constituent, while the matrix captures how it changes the mean- ing of neighboring words or phrases. This matrix-vector RNN can learn the meaning of operators in propositional logic and natural language. The model obtains state of the art performance on three different experiments: predicting fine-grained sentiment distributions of adverb-adjective pairs; classifying senti- ment labels of movie reviews and classifying semantic relationships such as cause-effect or topic-message between nouns using the syn- tactic path between them.},
address = {Stroudsburg, PA, USA},
author = {Socher, Richard and Huval, Brody and Manning, Christopher D. and Ng, Andrew Y.},
booktitle = {Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning},
file = {:Users/terrex/Documents/Mendeley Desktop/Socher et al/2012/Socher et al. - 2012 - Semantic compositionality through recursive matrix-vector spaces.pdf:pdf},
isbn = {9781937284435},
keywords = {Socher2012},
mendeley-tags = {Socher2012},
month = jul,
pages = {1201--1211},
publisher = {Association for Computational Linguistics},
series = {EMNLP-CoNLL '12},
title = {{Semantic compositionality through recursive matrix-vector spaces}},
url = {http://dl.acm.org/citation.cfm?id=2391084},
year = {2012}
}
@misc{Ruiz2001,
author = {Ruiz, Miguel \'{A}ngel and Pardo, Antonio},
file = {:Users/terrex/Documents/Mendeley Desktop/Ruiz, Pardo/2001/Ruiz, Pardo - 2001 - An\'{a}lisis discriminante el procedimiento Discriminante.pdf:pdf},
title = {{An\'{a}lisis discriminante: el procedimiento Discriminante}},
url = {http://pendientedemigracion.ucm.es/info/socivmyt/paginas/D\_departamento/materiales/analisis\_datosyMultivariable/23discr\_SPSS.pdf},
urldate = {2015-06-24},
year = {2001}
}
@article{Santorini1990,
abstract = {This manual addresses the linguistic issues that arise in connection with annotating texts by part of speech ("tagging"). Section 2 is an alphabetical list of the parts of speech encoded in the annotation systems of the Penn Treebank Project, along with their corresponding abbreviations ("tags") and some information concerning their definition. This section allows you to find an unfamiliar tag by looking up a familiar part of speech. Section 3 recapitulates the information in Section 2, but this time the information is alphabetically ordered by tags. This is the section to consult in order to find out what an unfamiliar tag means. Since the parts of speech are probably familiar to you from high school English, you should have little difficulty in assimilating the tags themselves. However, it is often quite difficult to decide which tag is appropriate in a particular context. The two sections 4 and 5 therefore include examples and guidelines on how to tag problematic cases. If you are uncertain about whether a given tag is correct or not, refer to these sections in order to ensure a consistently annotated text. Section 4 discusses parts of speech that are easily confused and gives guidelines on how to tag such cases, while Section 5 contains an alphabetical list of specific problematic words and collocations. Finally, Section 6 discusses some general tagging conventions. One general rule, however, is so important that we state it here. Many texts are not models of good prose, and some contain outright errors and slips of the pen. Do not be tempted to correct a tag to what it would be if the text were correct; rather, it is the incorrect word that should be tagged correctly.},
author = {Santorini, Beatrice},
file = {:Users/terrex/Documents/Mendeley Desktop/Santorini/1990/Santorini - 1990 - Part-of-Speech Tagging Guidelines for the Penn Treebank Project (3rd Revision).ps:ps;:Users/terrex/Documents/Mendeley Desktop/Santorini/1990/Santorini - 1990 - Part-of-Speech Tagging Guidelines for the Penn Treebank Project (3rd Revision).pdf:pdf},
journal = {Technical Reports (CIS)},
month = jul,
pages = {Paper 570},
title = {{Part-of-Speech Tagging Guidelines for the Penn Treebank Project (3rd Revision)}},
year = {1990}
}
@book{Loganathan2013,
abstract = {Elegantly built GUI applications are always a massive hit among users. PySide is an open source software project that provides Python bindings for the Qt cross-platform UI framework. Combining the power of Qt and Python, PySide provides easy access to the Qt framework for Python developers and also acts as an excellent rapid application development platform available on all major operating systems. This book aims to help you develop GUI applications easily using PySide. Python is easy to learn and use and its programs are relatively shorter than those written in other programming languages like C++ or Java. This book will introduce you to user interface programming in Python, allowing you to develop real-time applications in a shorter amount of time.},
author = {Loganathan, Venkateshwaran},
file = {:Users/terrex/Documents/Mendeley Desktop/Loganathan/2013/Loganathan - 2013 - PySide GUI application development.pdf:pdf},
isbn = {9781849699594},
pages = {140},
publisher = {Packt Publishing Ltd},
title = {{PySide GUI application development}},
url = {http://www.it-ebooks.info/book/3704/},
year = {2013}
}
@misc{wikipedia-es,
author = {Wikipedia},
title = {{Wikipedia en Espa\~{n}ol}},
url = {http://es.wikipedia.org/},
urldate = {2015-05-30},
year = {2015}
}
@misc{Rehurek,
author = {Rehurek, Radim},
keywords = {consulting,gensim,machine learning,programming,radim rehurek},
title = {{Word2vec Tutorial}},
url = {http://radimrehurek.com/2014/02/word2vec-tutorial/},
urldate = {2015-01-19}
}
@book{Bird2009,
abstract = {This book offers a highly accessible introduction to natural language processing, the field that supports a variety of language technologies, from predictive text and email filtering to automatic summarization and translation. With it, you'll learn how to write Python programs that work with large collections of unstructured text. You'll access richly annotated datasets using a comprehensive range of linguistic data structures, and you'll understand the main algorithms for analyzing the content and structure of written communication. Packed with examples and exercises, Natural Language Processing with Python will help you:Extract information from unstructured text, either to guess the topic or identify "named entities" Analyze linguistic structure in text, including parsing and semantic analysis Access popular linguistic databases, including WordNet and treebanks Integrate techniques drawn from fields as diverse as linguistics and artificial intelligence This book will help you gain practical skills in natural language processing using the Python programming language and the Natural Language Toolkit (NLTK) open source library. If you're interested in developing web applications, analyzing multilingual news sources, or documenting endangered languages - or if you're simply curious to have a programmer's perspective on how human language works - you'll find Natural Language Processing with Python both fascinating and immensely useful.},
author = {Bird, Steven and Klein, Ewan and Loper, Edward},
booktitle = {Text},
file = {:Users/terrex/Documents/Mendeley Desktop/Bird, Klein, Loper/2009/Bird, Klein, Loper - 2009 - Natural Language Processing with Python.pdf:pdf},
isbn = {9780596516499},
issn = {00992399},
pages = {479},
pmid = {12043876},
title = {{Natural Language Processing with Python}},
url = {http://www.nltk.org/book/},
volume = {43},
year = {2009}
}
@article{Pedregosa2011,
author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
file = {:Users/terrex/Documents/Mendeley Desktop/Pedregosa et al/2011/Pedregosa et al. - 2011 - Scikit-learn Machine Learning in Python.pdf:pdf},
journal = {Journal of Machine Learning Research},
pages = {2825--2830},
title = {{Scikit-learn: Machine Learning in Python}},
url = {http://scikit-learn.org/stable/user\_guide.html},
volume = {12},
year = {2011}
}
@misc{DiazSantos2015,
author = {{D\'{\i}az Santos}, Rosa},
title = {{Lenguaje natural, lenguaje artificial}},
url = {http://ficus.pntic.mec.es/rdis0006/lecciones/logica\_proposicional/lecciones/lenguaje natural.htm},
urldate = {2015-06-05}
}
@article{Porter1980,
author = {Porter, Martin F.},
doi = {10.1108/00330330610681286},
file = {:Users/terrex/Documents/Mendeley Desktop/Porter/1980/Porter - 1980 - An algorithm for suffix stripping.pdf:pdf;:Users/terrex/Documents/Mendeley Desktop/Porter/1980/Porter - 1980 - An algorithm for suffix stripping(2).pdf:pdf},
journal = {Program},
number = {3},
pages = {130--137},
publisher = {MCB UP Ltd},
title = {{An algorithm for suffix stripping}},
url = {http://tartarus.org/martin/PorterStemmer/},
volume = {14},
year = {1980}
}
@inproceedings{DBLP:journals/corr/MikolovSCCD13,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large num- ber of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alterna- tive to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of “Canada” and “Air” cannot be easily combined to obtain “Air Canada”. Motivated by this example,we present a simplemethod for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
archivePrefix = {arXiv},
arxivId = {1310.4546},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
booktitle = {NIPS},
eprint = {1310.4546},
file = {:Users/terrex/Documents/Mendeley Desktop/Mikolov et al/2013/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases and their Compositionality.pdf:pdf},
issn = {10495258},
pages = {1--9},
title = {{Distributed Representations of Words and Phrases and their Compositionality}},
url = {http://arxiv.org/abs/1310.4546},
volume = {abs/1310.4},
year = {2013}
}
@inproceedings{Socher2011a,
address = {Computer Science Department, Stanford University},
author = {Socher, Richard and Lin, Cliff Chiung-yu C.C.-Y. and Ng, Andrew Y. and Manning, Christopher D.},
booktitle = {Proceedings of the 28th International Conference on Machine Learning},
file = {:Users/terrex/Documents/Mendeley Desktop/Socher et al/2011/Socher et al. - 2011 - Parsing natural scenes and natural language with recursive neural networks.pdf:pdf},
keywords = {Socher2011a},
mendeley-tags = {Socher2011a},
pages = {129--136},
series = {ICML'11},
title = {{Parsing natural scenes and natural language with recursive neural networks}},
url = {http://fur.ly/0/Socher2011},
year = {2011}
}
@article{Karray2008,
author = {Karray, Fakhreddine and Alemzadeh, Milad and Saleh, Jamil Abou and Arab, Mo Nours},
file = {:Users/terrex/Documents/Mendeley Desktop/Karray et al/2008/Karray et al. - 2008 - Human-Computer Interaction Overview on State of the Art.pdf:pdf},
journal = {International Journal on Smart Sensing and Intelligent Systems},
month = mar,
number = {1},
title = {{Human-Computer Interaction: Overview on State of the Art}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.331.6558},
volume = {1},
year = {2008}
}
@book{Zubira2002,
author = {{De Zubira Samper}, Miguel},
isbn = {9789589405062},
publisher = {Fondo de publicaciones Merani},
title = {{Teor\'{\i}a de las seis lecturas}},
url = {https://books.google.es/books?id=PFfmoAEACAAJ},
year = {2002}
}
@article{Edmondson1978,
author = {Edmondson, Jerold A.},
doi = {10.1016/0024-3841(78)90031-1},
file = {:Users/terrex/Documents/Mendeley Desktop/Edmondson/1978/Edmondson - 1978 - Formal Semantics of Natural Language (book review).pdf:pdf},
issn = {00243841},
journal = {Lingua},
month = aug,
number = {3-4},
pages = {355--374},
title = {{Formal Semantics of Natural Language (book review)}},
url = {http://www.sciencedirect.com/science/article/pii/0024384178900311},
volume = {45},
year = {1978}
}
@misc{GooglePredictionAPI,
author = {Google},
title = {{Creating a Sentiment Analysis Model - Prediction API — Google Cloud Platform}},
url = {https://cloud.google.com/prediction/docs/sentiment\_analysis},
urldate = {2015-06-17},
year = {2015}
}
@article{Domingos2012,
abstract = {MACHINE LEARNING SYSTEMS automatically learn programs from data. This is often a very attractive alternative to manually constructing them, and in the last decade the use of machine learning has spread rapidly throughout computer science and beyond. Machine learning is used in Web search, spam filters, recommender systems, ad placement, credit scoring, fraud detection, stock trading, drug design, and many other applications. A recent report from the McKinsey Global Institute asserts that machine learning (a.k.a. data mining or predictive analytics) will be the driver of the next big wave of innovation. Several fine textbooks are available to interested practitioners and researchers (for example, Mitchell and Witten et al.). However, much of the ``folk knowledge'' that is needed to successfully develop machine learning applications is not readily available in them. As a result, many machine learning projects take much longer than necessary or wind up producing less-than-ideal results. Yet much of this folk knowledge is fairly easy to communicate. This is the purpose of this article.},
author = {Domingos, Pedro},
doi = {10.1145/2347736.2347755},
file = {:Users/terrex/Documents/Mendeley Desktop/Domingos/2012/Domingos - 2012 - A few useful things to know about machine learning.pdf:pdf;:Users/terrex/Documents/Mendeley Desktop/Domingos/2012/Domingos - 2012 - A few useful things to know about machine learning(2).pdf:pdf},
isbn = {0001-0782},
issn = {00010782},
journal = {Communications of the ACM},
month = oct,
number = {10},
pages = {78},
title = {{A few useful things to know about machine learning}},
volume = {55},
year = {2012}
}
@book{Russell2003,
author = {Russell, Stuart J. and Norvig, Peter},
edition = {2nd},
file = {:Users/terrex/Documents/Mendeley Desktop/Russell, Norvig/1996/Russell, Norvig - 1996 - Artificial Intelligence A Modern Approach.pdf:pdf},
isbn = {0137903952},
publisher = {Pearson Education},
title = {{Artificial Intelligence: A Modern Approach}},
year = {2003}
}
@phdthesis{Bobrow1964,
abstract = {This paper describes a computer program which accepts and «understands» a comfortable, but restricted set of one natural language, English. Certain difficulties are inherent in this problem of making a machine «understand» English. Within the limited framework of the subject matter understood by the program, many of these problems are solved or circumvented. I shall describe these problems and my solutions, and point out those solutions which I feel have general applicability. I will also indicate which must be replaced by more general methods to be really useful, and give my ideas about what general solutions to these particular problems might entail.},
author = {Bobrow, Daniel G.},
doi = {1721.1/5922},
file = {:Users/terrex/Documents/Mendeley Desktop/Bobrow/1964/Bobrow - 1964 - Natural Language Input for a Computer Problem Solving System.pdf:pdf},
keywords = {Bobrow1964},
mendeley-tags = {Bobrow1964},
publisher = {MIT},
title = {{Natural Language Input for a Computer Problem Solving System}},
year = {1964}
}
@inproceedings{Hearst1992,
abstract = {We describe a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text. Two goals motivate the approach: (i) avoidance of the need for pre-encoded knowledge and (ii) applicability across a wide range of text. We identify a set of lexicosyntactic patterns that are easily recognizable, that occur frequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest. We describe a method for discovering these patterns and suggest that other lexical relations will also be acquirable in this way. A subset of the acquisition algorithm is implemented and the results are used to augment and critique the structure of a large hand-built thesaurus. Extensions and applications to areas such as information retrieval are suggested. 1 Introduction Currently there is much interest in the automatic acquisition of lexical syntax and semantics, with the goal of building up large lexicons for natural language processing. Projects ...},
annote = {http://people.ischool.berkeley.edu/\~{}hearst/},
author = {Hearst, Marti A.},
booktitle = {Proceedings of the 14th International Conference on Computational Linguistics},
doi = {10.1.1.36.701},
file = {:Users/terrex/Documents/Mendeley Desktop/Hearst/1992/Hearst - 1992 - Automatic Acquisition of Hyponyms from Large Text Corpora.pdf:pdf},
pages = {539--545},
title = {{Automatic Acquisition of Hyponyms from Large Text Corpora}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.36.701},
year = {1992}
}
@article{DBLP:journals/corr/LeM14,
abstract = {Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, "powerful," "strong" and "Paris" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.},
archivePrefix = {arXiv},
arxivId = {1405.4053},
author = {Le, Quoc V. and Mikolov, Tomas},
eprint = {1405.4053},
file = {:Users/terrex/Documents/Mendeley Desktop/Le, Mikolov/2014/Le, Mikolov - 2014 - Distributed Representations of Sentences and Documents.pdf:pdf},
isbn = {9781634393973},
journal = {Proceedings of the 31st International Conference on Machine Learning},
pages = {1188--1196},
series = {ICML'14},
title = {{Distributed Representations of Sentences and Documents}},
url = {http://arxiv.org/abs/1405.4053},
volume = {32},
year = {2014}
}
@inproceedings{Socher2013,
abstract = {Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80\% up to 85.4\%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7\%, an improvement of 9.7\% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.},
author = {Socher, Richard and Perelygin, Alex and Wu, Jean Y. and Chuang, Jason and Manning, Christopher D. and Ng, Andrew Y. and Potts, Christopher},
booktitle = {Proceedings of the 2013 conference on Empirical Methods in Natural Language Processing},
file = {:Users/terrex/Documents/Mendeley Desktop/Socher et al/2013/Socher et al. - 2013 - Recursive deep models for semantic compositionality over a sentiment treebank.pdf:pdf},
month = oct,
pages = {1631--1642},
publisher = {Association for Computational Linguistics},
series = {EMNLP '13},
title = {{Recursive deep models for semantic compositionality over a sentiment treebank}},
url = {http://nlp.stanford.edu/~socherr/EMNLP2013\_RNTN.pdf},
year = {2013}
}
@book{Perkins2014,
abstract = {This book will show you the essential techniques of text and language processing. Starting with tokenization, stemming, and the WordNet dictionary, you'll progress to part-of-speech tagging, phrase chunking, and named entity recognition. You'll learn how various text corpora are organized, as well as how to create your own custom corpus. Then, you'll move onto text classification with a focus on sentiment analysis. And because NLP can be computationally expensive on large bodies of text, you'll try a few methods for distributed text processing. Finally, you'll be introduced to a number of other small but complementary Python libraries for text analysis, cleaning, and parsing.},
author = {Perkins, Jacob},
file = {:Users/terrex/Documents/Mendeley Desktop/Perkins/2014/Perkins - 2014 - Python 3 Text Processing with NLTK 3 Cookbook.pdf:pdf},
isbn = {978-1-78216-785-3},
publisher = {Packt Publishing Ltd},
title = {{Python 3 Text Processing with NLTK 3 Cookbook}},
url = {http://it-ebooks.info/book/4469/},
year = {2014}
}
@article{Zhang20151857,
abstract = {Since the booming development of e-commerce in the last decade, the researchers have begun to pay more attention to extract the valuable information from consumers comments. Sentiment classification, which focuses on classify the comments into positive class and negative class according to the polarity of sentiment, is one of the studies. Machine learning-based method for sentiment classification becomes mainstream due to its outstanding performance. Most of the existing researches are centered on the extraction of lexical features and syntactic features, while the semantic relationships between words are ignored. In this paper, in order to get the semantic features, we propose a method for sentiment classification based on word2vec and SVMperf. Our research consists of two parts of work. First of all, we use word2vec to cluster the similar features for purpose of showing the capability of word2vec to capture the semantic features in selected domain and Chinese language. And then, we train and classify the comment texts using word2vec again and SVMperf. In the process, the lexicon-based and part-of-speech-based feature selection methods are respectively adopted to generate the training file. We conduct the experiments on the data set of Chinese comments on clothing products. The experimental results show the superior performance of our method in sentiment classification.},
author = {Zhang, Dongwen and Xu, Hua and Su, Zengcai and Xu, Yunfeng},
doi = {10.1016/j.eswa.2014.09.011},
file = {:Users/terrex/Documents/Mendeley Desktop/Zhang et al/2015/Zhang et al. - 2015 - Chinese comments sentiment classification based on word2vec and SVMperf.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Semantic features,Sentiment classification,Word2vec,\{SVMperf\}},
number = {4},
pages = {1857--1863},
title = {{Chinese comments sentiment classification based on word2vec and SVMperf}},
url = {http://www.sciencedirect.com/science/article/pii/S0957417414005508},
volume = {42},
year = {2015}
}
@book{Manning2008,
abstract = {Introduction to Information Retrieval is the first textbook with a coherent treatment of classical and web information retrieval, including web search and the related areas of text classification and text clustering. Written from a computer science perspective, it gives an up-to-date treatment of all aspects of the design and implementation of systems for gathering, indexing, and searching documents; methods for evaluating systems; and an introduction to the use of machine learning methods on text collections. Designed as the primary text for a graduate or advanced undergraduate course in information retrieval, the book will also interest researchers and professionals. A complete set of lecture slides and exercises that accompany the book are available on the web.},
author = {Manning, Christopher D. and Raghavan, Prabhakar and Sch\"{u}tze, Hinrich},
booktitle = {Journal of the American Society for Information Science and Technology},
doi = {10.1002/asi.21234},
file = {:Users/terrex/Documents/Mendeley Desktop/Manning, Raghavan, Sch\"{u}tze/2008/Manning, Raghavan, Sch\"{u}tze - 2008 - Introduction to Information Retrieval.pdf:pdf},
isbn = {0521865719},
issn = {15322890},
pages = {496},
pmid = {12049181},
publisher = {Cambridge University Press},
title = {{Introduction to Information Retrieval}},
url = {http://nlp.stanford.edu/IR-book/},
volume = {1},
year = {2008}
}
@inproceedings{Socher2011b,
abstract = {We introduce a novel machine learning framework based on recursive autoencoders for sentence-level prediction of sentiment label distributions. Our method learns vector space representations for multi-word phrases. In sentiment prediction tasks these representations outperform other state-of-the-art approaches on commonly used datasets, such as movie reviews, without using any pre-defined sentiment lexica or polarity shifting rules. We also evaluate the model's ability to predict sentiment distributions on a new dataset based on confessions from the experience project. The dataset consists of personal user stories annotated with multiple labels which, when aggregated, form a multinomial distribution that captures emotional reactions. Our algorithm can more accurately predict distributions over such labels compared to several competitive baselines.},
address = {Stroudsburg, PA, USA},
annote = {http://www.aclweb.org/anthology/D11-1014},
author = {Socher, Richard and Pennington, Jeffrey and Huang, Eric H. and Ng, Andrew Y. and Manning, Christopher D.},
booktitle = {Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing},
file = {:Users/terrex/Documents/Mendeley Desktop/Socher et al/2011/Socher et al. - 2011 - Semi-supervised Recursive Autoencoders for Predicting Sentiment Distributions.pdf:pdf},
isbn = {978-1-937284-11-4},
keywords = {Socher2011b},
mendeley-tags = {Socher2011b},
pages = {151--161},
publisher = {Association for Computational Linguistics},
series = {EMNLP '11},
title = {{Semi-supervised Recursive Autoencoders for Predicting Sentiment Distributions}},
url = {http://dl.acm.org/citation.cfm?id=2145450},
year = {2011}
}
@misc{MartinMateos2013,
author = {{Mart\'{\i}n Mateos}, F. J. and {Ruiz Reina}, J. L.},
file = {:Users/terrex/Documents/Mendeley Desktop/Mart\'{\i}n Mateos, Ruiz Reina/2013/Mart\'{\i}n Mateos, Ruiz Reina - 2013 - Procesamiento del lenguaje natural.pdf:pdf},
publisher = {Dpto. Ciencias de la Computaci\'{o}n e Inteligencia Artificial. Universidad de Sevilla},
title = {{Procesamiento del lenguaje natural}},
url = {http://www.cs.us.es/cursos/ia2/temas/tema-06.pdf},
year = {2013}
}
@book{Manning1999,
abstract = {The Handbook of Natural Language Processing, Second Edition presents practical tools and techniques for implementing natural language processing in computer systems. Along with removing outdated material, this edition updates every chapter and expands the content to include emerging areas, such as sentiment analysis. New to the Second Edition Greater prominence of statistical approaches New applications section Broader multilingual scope to include Asian and European languages, along with English An actively maintained wiki that provides online resources, supplementary information, and up-to-date developments Divided into three sections, the book first surveys classical techniques, including both symbolic and empirical approaches. The second section focuses on statistical approaches in natural language processing. In the final section of the book, each chapter describes a particular class of application, from Chinese machine translation to information visualization to ontology construction to biomedical text mining. Fully updated with the latest developments in the field, this comprehensive, modern handbook emphasizes how to implement practical language processing tools in computational systems.},
author = {Manning, Christopher D. and Sch\"{u}tze, Hinrich},
booktitle = {Reading},
file = {:Users/terrex/Documents/Mendeley Desktop/Manning, Sch\"{u}tze/1999/Manning, Sch\"{u}tze - 1999 - Foundations of Statistical Natural Language Processing.pdf:pdf;:Users/terrex/Documents/Mendeley Desktop/Manning, Sch\"{u}tze/1999/Manning, Sch\"{u}tze - 1999 - Foundations of Statistical Natural Language Processing(2).pdf:pdf},
isbn = {9780262133609},
pages = {620},
publisher = {MIT press},
title = {{Foundations of Statistical Natural Language Processing}},
url = {http://nlp.stanford.edu/fsnlp/},
year = {1999}
}
@book{Keenan2009,
abstract = {A volume of studies in natural language semantics which brings together work by philosophers, logicians and linguists. The main topics treated are: quantification and reference in natural language; the relations between formal logic, programming languages and natural language; pragmatics and discourse meaning; surface syntax and logical meaning. The volume derives from a colloquium organised in 1973 by the Kings College Research Centre, Cambridge and the papers have been edited for publication by Professor Keenan. It is hoped that the collection will make available some of the best work in this fast-moving field and will stimulate further progress by juxtaposing the different approaches and interests represented here.},
author = {Keenan, Eduard L.},
isbn = {9780521111119},
pages = {492},
publisher = {Cambridge University Press},
title = {{Formal Semantics of Natural Language}},
url = {https://books.google.es/books?id=3o5bPwAACAAJ},
year = {2009}
}
@misc{wikipedia-en,
author = {Wikipedia},
title = {{Wikipedia in English}},
url = {http://en.wikipedia.org/},
urldate = {2015-05-30},
year = {2015}
}
@misc{Rehurekb,
author = {Rehurek, Radim},
title = {{gensim: models.word2vec – Deep learning with word2vec}},
url = {http://radimrehurek.com/gensim/models/word2vec.html},
urldate = {2015-01-19}
}
@phdthesis{Handler2014,
abstract = {This thesis performs an empirical analysis of Word2Vec by comparing its output to WordNet, a well-known, human-curated lexical database. It finds that Word2Vec tends to uncover more of certain types of semantic relations than others -- with Word2Vec returning more hypernyms, synonomyns and hyponyms than hyponyms or holonyms. It also shows the probability that neighbors separated by a given cosine distance in Word2Vec are semantically related in WordNet. This result both adds to our understanding of the still-unknown Word2Vec and helps to benchmark new semantic tools built from word vectors.},
author = {Handler, Abram},
booktitle = {University of New Orleans Theses and Dissertations},
file = {:Users/terrex/Documents/Mendeley Desktop/Handler/2014/Handler - 2014 - An empirical study of semantic similarity in WordNet and Word2Vec.pdf:pdf},
month = dec,
school = {University of New Orleans},
title = {{An empirical study of semantic similarity in WordNet and Word2Vec}},
url = {http://scholarworks.uno.edu/td/1922},
year = {2014}
}
@phdthesis{Socher2014,
author = {Socher, Richard},
file = {:Users/terrex/Documents/Mendeley Desktop/Socher/2014/Socher - 2014 - Recursive Deep Learning for Natural Language Processing and Computer Vision.pdf:pdf},
keywords = {Socher2014},
mendeley-tags = {Socher2014},
month = aug,
publisher = {Stanford University},
school = {Stanford University},
title = {{Recursive Deep Learning for Natural Language Processing and Computer Vision}},
url = {http://nlp.stanford.edu/~socherr/thesis.pdf},
year = {2014}
}
@article{Porter2006,
abstract = {Purpose – The automatic removal of suffixes from words in English is of particular interest in the field of information retrieval. This work was originally published in Program in 1980 and is republished as part of a series of articles commemorating the 40th anniversary of the journal.Design/methodology/approach – An algorithm for suffix stripping is described, which has been implemented as a short, fast program in BCPL.Findings – Although simple, it performs slightly better than a much more elaborate system with which it has been compared. It effectively works by treating complex suffixes as compounds made up of simple suffixes, and removing the simple suffixes in a number of steps. In each step the removal of the suffix is made to depend upon the form of the remaining stem, which usually involves a measure of its syllable length.Originality/value – The piece provides a useful historical document on information retrieval.},
author = {Porter, Martin F.},
doi = {10.1108/00330330610681286},
file = {:Users/terrex/Documents/Mendeley Desktop/Porter/1980/Porter - 1980 - An algorithm for suffix stripping(2).pdf:pdf},
journal = {Program},
number = {3},
pages = {211--218},
title = {{An algorithm for suffix stripping}},
url = {http://www.emeraldinsight.com/doi/abs/10.1108/00330330610681286},
volume = {40},
year = {2006}
}
@book{Russell1995,
author = {Russell, Stuart J. and Norvig, Peter},
edition = {1st},
file = {:Users/terrex/Documents/Mendeley Desktop/Russell, Norvig/1995/Russell, Norvig - 1995 - Artificial Intelligence A modern approach(2).pdf:pdf;:Users/terrex/Documents/Mendeley Desktop/Russell, Norvig/1995/Russell, Norvig - 1995 - Artificial Intelligence A modern approach.pdf:pdf},
publisher = {Prentice-Hall},
title = {{Artificial Intelligence: A modern approach}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.259.8854\&rep=rep1\&type=pdf},
year = {1995}
}
@misc{word2vec,
author = {Mikolov, Tomas},
keywords = {word2vec},
mendeley-tags = {word2vec},
title = {{word2vec - Tool for computing continuous distributed representations of words}},
url = {https://code.google.com/p/word2vec/},
urldate = {2014-10-07}
}
@book{VanRijsbergen1980,
author = {{Van Rijsbergen}, Cornelis J. and Robertson, Stephen Edward and Porter, Martin F.},
publisher = {Computer Laboratory, University of Cambridge},
title = {{New models in probabilistic information retrieval}},
url = {http://www.sigir.org/files/museum/pub-21/},
year = {1980}
}
@book{Chomsky1965,
abstract = {Beginning in the mid-fifties and emanating largely form MIT, and approach was developed to linguistic theory and to the study of the structure of particular languages that diverges in many respects from modern linguistics. Although this approach is connected to the traditional study of languages, it differs enough in its specific conclusions about the structure and in its specific conclusions about the structure of language to warrant a name, "generative grammar."Various deficiencies have been discovered in the first attempts to formulate a theory of transformational generative grammar and in the descriptive analysis of particular languages that motivated these formulations. At the same time, it has become apparent that these formulations can be extended and deepened.The major purpose of this book is to review these developments and to propose a reformulation of the theory of transformational generative grammar that takes them into account. The emphasis in this study is syntax; semantic and phonological aspects of the language structure are discussed only insofar as they bear on syntactic theory.},
author = {Chomsky, Noam},
booktitle = {Aspects of the Theory of Syntax},
doi = {10.1016/0732-118X(86)90008-5},
isbn = {0262530074},
issn = {0732118X},
number = {no 11},
pages = {251},
pmid = {3586770},
title = {{Aspects of the Theory of Syntax}},
volume = {119},
year = {1965}
}
@inproceedings{Gabrilovich2005,
abstract = {We enhance machine learning algorithms for text categorization with generated features based on domain-specific and common-sense knowledge. This knowledge is represented using publicly available ontologies that contain hundreds of thousands of concepts, such as the Open Directory; these ontologies are further enriched by several orders of magnitude through controlled Web crawling. Prior to text categorization, a feature generator analyzes the documents and maps them onto appropriate ontology concepts, which in turn induce a set of generated features that augment the standard bag of words. Feature generation is accomplished through contextual analysis of document text, implicitly performing word sense disambiguation. Coupled with the ability to generalize concepts using the ontology, this approach addresses the two main problems of natural language processing---synonymy and polysemy. Categorizing documents with the aid of knowledge-based features leverages information that cannot be deduced from the documents alone. Experimental results confirm improved performance, breaking through the plateau previously reached in the field.},
author = {Gabrilovich, Evgeniy and Markovitch, Shaul},
booktitle = {Proceedings of the 19th International Joint Conference for Artificial Intelligence},
file = {:Users/terrex/Documents/Mendeley Desktop/Gabrilovich, Markovitch/2005/Gabrilovich, Markovitch - 2005 - Feature Generation for Text Categorization Using World Knowledge.pdf:pdf},
keywords = {Common-Sense Knowledge,Feature Generation,Gabrilovich2005,Informat},
mendeley-tags = {Gabrilovich2005},
pages = {1048--1053},
title = {{Feature Generation for Text Categorization Using World Knowledge}},
url = {http://www.cs.technion.ac.il/~shaulm/papers/pdf/Gabrilovich-Markovitch-ijcai2005.pdf},
year = {2005}
}
@inproceedings{Pang2002,
abstract = {We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classification problem more challenging.},
annote = {http://www.cs.cornell.edu/home/llee/papers/sentiment.pdf},
archivePrefix = {arXiv},
arxivId = {cs/0205070},
author = {Pang, Bo and Lee, Lillian and Vaithyanathan, Shivakumar},
booktitle = {Proceedings of the ACL-02 conference on Empirical Methods in Natural Language Processing},
doi = {10.3115/1118693.1118704},
eprint = {0205070},
file = {:Users/terrex/Documents/Mendeley Desktop/Pang, Lee, Vaithyanathan/2002/Pang, Lee, Vaithyanathan - 2002 - Thumbs Up Sentiment Classification Using Machine Learning Techniques.pdf:pdf},
keywords = {Pang2002},
mendeley-tags = {Pang2002},
pages = {79--86},
primaryClass = {cs},
publisher = {Association for Computational Linguistics},
series = {EMNLP'02},
title = {{Thumbs Up? Sentiment Classification Using Machine Learning Techniques}},
volume = {10},
year = {2002}
}
@book{Handke2012,
address = {Tubingen, DEU},
author = {Handke, J\"{u}rgen},
file = {:Users/terrex/Documents/Mendeley Desktop/Handke/2012/Handke - 2012 - Natural Language Processing Structure of the Lexicon Human versus Machine.pdf:pdf},
isbn = {9783110907865},
keywords = {Computational linguistics.,Lexicology -- Data processing.},
publisher = {Walter de Gruyter},
title = {{Natural Language Processing : Structure of the Lexicon : Human versus Machine}},
url = {http://site.ebrary.com/lib/bibucascb/docDetail.action?docID=10755060},
year = {2012}
}
@misc{wikipedia:curse-dimens,
author = {Wikipedia},
title = {{Curse of dimensionality}},
url = {http://en.wikipedia.org/wiki/Curse\_of\_dimensionality},
urldate = {2014-11-09},
year = {2014}
}
@misc{Bouckaert2002,
author = {Bouckaert, Remco R. and Frank, Eibe and Kirkby, Richard and Reutemann, Peter and Seewald, Alex and Scuse, David},
file = {:Users/terrex/Documents/Mendeley Desktop/Bouckaert et al/2002/Bouckaert et al. - 2002 - WEKA Manual for Version 3-7-2.pdf:pdf},
title = {{WEKA Manual for Version 3-7-2}},
url = {http://netcologne.dl.sourceforge.net/project/weka/documentation/3.7.x/WekaManual-3-7-12.pdf},
year = {2002}
}
@inproceedings{Pang2004,
abstract = {Sentiment analysis seeks to identify the viewpoint(s) underlying a text span; an example application is classifying a movie review as "thumbs up" or "thumbs down". To determine this sentiment polarity, we propose a novel machine-learning method that applies text-categorization techniques to just the subjective portions of the document. Extracting these portions can be implemented using efficient techniques for finding minimum cuts in graphs; this greatly facilitates incorporation of cross-sentence contextual constraints.},
archivePrefix = {arXiv},
arxivId = {cs/0409058},
author = {Pang, Bo and Lee, Lillian},
booktitle = {Proceedings of the 42nd Meeting of the Association for Computational Linguistics},
doi = {10.3115/1218955.1218990},
eprint = {0409058},
file = {:Users/terrex/Documents/Mendeley Desktop/Pang, Lee/2004/Pang, Lee - 2004 - A Sentimental Education Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts.pdf:pdf},
issn = {01403664},
keywords = {Pang2004,analysis,sentiment},
mendeley-tags = {Pang2004},
pages = {271----278},
primaryClass = {cs},
series = {ACL'04},
title = {{A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts}},
year = {2004}
}
@inproceedings{Turian2010,
abstract = {If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih \& Hinton, 2009) embeddings of words on both NER and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further improvements by combining different word representations. You can download our word features, for off-the-shelf use in existing NLP systems, as well as our code, here: http://metaoptimize.com/projects/wordreprs/},
author = {Turian, Joseph and Ratinov, Lev and Bengio, Yoshua},
booktitle = {Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics},
file = {:Users/terrex/Documents/Mendeley Desktop/Turian, Ratinov, Bengio/2010/Turian, Ratinov, Bengio - 2010 - Word Representations A Simple and General Method for Semi-supervised Learning.pdf:pdf},
isbn = {9781617388088},
keywords = {Turian2010},
mendeley-tags = {Turian2010},
pages = {384--394},
series = {ACL'10},
title = {{Word Representations: A Simple and General Method for Semi-supervised Learning}},
url = {http://www.iro.umontreal.ca/~lisa/pointeurs/turian-wordrepresentations-acl10.pdf},
year = {2010}
}
@mastersthesis{Gerber2011,
author = {Gerber, Matthew},
file = {:Users/terrex/Documents/Mendeley Desktop/Gerber/2011/Gerber - 2011 - Semantic Role Labeling of Implicit Arguments for Nominal Predicates.pdf:pdf},
month = aug,
school = {Michigan State University},
title = {{Semantic Role Labeling of Implicit Arguments for Nominal Predicates}},
type = {phd},
year = {2011}
}
@misc{JurgenBocklage-RyannelandJohanThelin,
author = {Bocklage-Ryannel, J\"{u}rgen and Thelin, Johan},
title = {{QmlBook}},
url = {https://qmlbook.github.io/index.html},
urldate = {2015-04-08}
}
@misc{Rai2011,
author = {Rai, Piyush},
booktitle = {2011},
file = {:Users/terrex/Documents/Mendeley Desktop/Rai/Unknown/Rai - Unknown - Data Clustering K-means and Hierarchical Clustering.pdf:pdf},
title = {{Data Clustering: K-means and Hierarchical Clustering}},
url = {http://www.cs.utah.edu/~piyush/teaching/4-10-print.pdf},
urldate = {2015-05-31}
}
@book{Jimenez2004,
author = {{Jim\'{e}nez Mill\'{a}n}, Jos\'{e} Antonio},
isbn = {978-84-7786-383-0},
pages = {273},
publisher = {Servicio de publicaciones de la Universidad de C\'{a}diz},
title = {{Compiladores y procesadores de lenguajes}},
year = {2004}
}
@book{Baeza-Yates2011,
abstract = {Comprehensive guide to information retrieval by leading international experts including web retrievel, web crawling, open source search engines and user interfaces.},
author = {Baeza-Yates, Ricardo and Ribeiro-Neto, Berthier},
booktitle = {Information Retrieval},
file = {:Users/terrex/Documents/Mendeley Desktop/Baeza-Yates, Ribeiro-Neto/2011/Baeza-Yates, Ribeiro-Neto - 2011 - Modern Information Retrieval The Concepts and Technology behind Search(2).pdf:pdf;:Users/terrex/Documents/Mendeley Desktop/Baeza-Yates, Ribeiro-Neto/2011/Baeza-Yates, Ribeiro-Neto - 2011 - Modern Information Retrieval The Concepts and Technology behind Search.pdf:pdf},
isbn = {0321416910},
pages = {944},
title = {{Modern Information Retrieval: The Concepts and Technology behind Search}},
url = {http://www.amazon.com/Modern-Information-Retrieval-Concepts-Technology/dp/0321416910},
volume = {82},
year = {2011}
}
@misc{Jurafsky2015,
author = {Jurafsky, Dan and Manning, Christopher D.},
title = {{Natural Language Processing - Coursera class}},
url = {https://class.coursera.org/nlp/lecture/25},
urldate = {2015-06-22},
year = {2015}
}
@misc{MorenoVelo2010,
author = {{Moreno Velo}, Francisco Jos\'{e}},
file = {:Users/terrex/Documents/Mendeley Desktop/Moreno Velo/2010/Moreno Velo - 2010 - Tema 3 Fundamentos de la Teor\'{\i}a de Gram\'{a}ticas Formales.pdf:pdf},
title = {{Tema 3: Fundamentos de la Teor\'{\i}a de Gram\'{a}ticas Formales}},
url = {http://www.uhu.es/francisco.moreno/talf/docs/tema3.pdf},
urldate = {2015-06-02},
year = {2010}
}
@book{Summerfield2008,
author = {Summerfield, Mark},
file = {:Users/terrex/Documents/Mendeley Desktop/Summerfield/2008/Summerfield - 2008 - Rapid GUI programming with Python and Qt the definitive guide to PyQt programming.pdf:pdf},
isbn = {9780132354189},
keywords = {Användargränssnitt,Python (programspråk),Qt (Electronic resource)},
publisher = {Prentice Hall},
title = {{Rapid GUI programming with Python and Qt: the definitive guide to PyQt programming}},
url = {http://fp5qq3tk5q.search.serialssolutions.com?ctx\_ver=Z39.88-2004\&ctx\_enc=info:ofi/enc:UTF-8\&rfr\_id=info:sid/summon.serialssolutions.com\&rft\_val\_fmt=info:ofi/fmt:kev:mtx:book\&rft.genre=book\&rft.title=Rapid+GUI+programming+with+Python+and+Qt\&rft.au=S},
year = {2008}
}
@misc{Rehureka,
author = {Rehurek, Radim},
keywords = {consulting,gensim,machine learning,programming,radim rehurek},
title = {{Performance Shootout of Nearest Neighbours: Querying}},
url = {http://radimrehurek.com/2014/01/performance-shootout-of-nearest-neighbours-querying/},
urldate = {2015-01-19}
}
@book{Russell2009,
address = {Upper Saddle River, NJ, USA},
author = {Russell, Stuart J. and Norvig, Peter},
edition = {3rd},
file = {:Users/terrex/Documents/Mendeley Desktop/Russell, Norvig/2009/Russell, Norvig - 2009 - Artificial Intelligence A Modern Approach.pdf:pdf},
isbn = {0136042597, 9780136042594},
publisher = {Prentice Hall Press},
title = {{Artificial Intelligence: A Modern Approach}},
year = {2009}
}
@misc{TextMiningOnline2015,
author = {Desconocido},
title = {{Text Mining Online | Text Analysis Online | Text Processing Online}},
url = {http://textminingonline.com},
urldate = {2015-05-28},
year = {2015}
}
@book{Zecchetto2003,
author = {Zecchetto, Victorino},
file = {:Users/terrex/Documents/Mendeley Desktop/Zecchetto/2003/Zecchetto - 2003 - La danza de los signos nociones de semi\'{o}tica general.pdf:pdf},
isbn = {9789871004195},
publisher = {La Cruj\{\'{\i}\}a},
series = {Colecci\{\'{o}\}n Inclusiones: Serie Categor\{\'{\i}\}as},
title = {{La danza de los signos: nociones de semi\'{o}tica general}},
url = {https://books.google.es/books?id=sbHjAAAAMAAJ},
year = {2003}
}
@article{Hilbert2011,
abstract = {We estimated the world's technological capacity to store, communicate, and compute information, tracking 60 analog and digital technologies during the period from 1986 to 2007. In 2007, humankind was able to store 2.9 × 10(20) optimally compressed bytes, communicate almost 2 × 10(21) bytes, and carry out 6.4 × 10(18) instructions per second on general-purpose computers. General-purpose computing capacity grew at an annual rate of 58\%. The world's capacity for bidirectional telecommunication grew at 28\% per year, closely followed by the increase in globally stored information (23\%). Humankind's capacity for unidirectional information diffusion through broadcasting channels has experienced comparatively modest annual growth (6\%). Telecommunication has been dominated by digital technologies since 1990 (99.9\% in digital format in 2007), and the majority of our technological memory has been in digital format since the early 2000s (94\% digital in 2007).},
author = {Hilbert, Martin and L\'{o}pez, Priscila},
doi = {10.1126/science.1200970},
file = {:Users/terrex/Documents/Mendeley Desktop/Hilbert, L\'{o}pez/2011/Hilbert, L\'{o}pez - 2011 - The world's technological capacity to store, communicate, and compute information.pdf:pdf},
isbn = {1095-9203},
issn = {0036-8075},
journal = {Science (New York, N.Y.)},
number = {6025},
pages = {60--65},
pmid = {21310967},
title = {{The world's technological capacity to store, communicate, and compute information}},
volume = {332},
year = {2011}
}
@misc{Gallego2008,
author = {Gallego, Angel J.},
booktitle = {Teorema: Revista internacional de filosof\'{\i}a},
file = {:Users/terrex/Documents/Mendeley Desktop/Gallego/2008/Gallego - 2008 - La jerarqu\'{\i}a de Chomsky y la facultad del lenguaje consecuencias para la variaci\'{o}n y la evoluci\'{o}n.pdf:pdf},
issn = {0210-1602},
keywords = {Filosof\'{\i}a. Etica,Grupo A,Grupo B,Humanidades},
language = {spa},
number = {2},
pages = {47--60},
publisher = {KRK Ediciones},
title = {{La jerarqu\'{\i}a de Chomsky y la facultad del lenguaje: consecuencias para la variaci\'{o}n y la evoluci\'{o}n}},
url = {http://dialnet.unirioja.es/servlet/articulo?codigo=2580734},
volume = {27},
year = {2008}
}
